{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e59e78d3",
      "metadata": {
        "id": "e59e78d3"
      },
      "source": [
        "This project is to use the learnings from the Torchvision Object Detection Finetuning Tutorial to have the model predict my dogs Kal and Jinse.\n",
        "\n",
        "I started by downloading photos of my two dogs that were in many different lighting and positions. At the end I had about 80 photos.\n",
        "\n",
        "I used CVAT.ai to label the images for masks and bounding boxes\n",
        "\n",
        "Steps to complete in this model creation\n",
        "1. Read annotations, images and masks from PASCAL VOC 1.1 format\n",
        "2. Create Dataset and Dataloader for training and test data\n",
        "3. Pull pre-trained model\n",
        "4. Modify the last layer to work to identify Kal And Jinse\n",
        "5. Train the model layer to identify Kal and Jinse for a few epochs.\n",
        "6. Test functionality of the model\n",
        "7. Adjust as necessary based on results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "269b4510",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-20T20:48:22.477958Z",
          "start_time": "2024-05-20T20:48:21.061672Z"
        },
        "id": "269b4510"
      },
      "outputs": [],
      "source": [
        "# Investigate out data\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "image = read_image(\"Data/KalAndJinseIdentifying/Images/PXL_20220822_235544436.jpg\")\n",
        "mask = read_image(\"Data/KalAndJinseIdentifying/SegmentationClass/PXL_20220822_235544436.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6860a9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-20T20:48:22.588109Z",
          "start_time": "2024-05-20T20:48:22.588101Z"
        },
        "id": "0a6860a9"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(8,4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Image\")\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Mask\")\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(mask.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d02fa4",
      "metadata": {
        "id": "59d02fa4"
      },
      "source": [
        "Get Item should return the image and the target value. Target will have boxes, masks, labels, image_id, area and iscrowd.\n",
        "\n",
        "target = {}\n",
        "target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size = F.get_size(img))\n",
        "target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "target[\"labels\"] = labels\n",
        "target[\"image_id\"] = image_id\n",
        "target[\"area\"] = area\n",
        "target[\"iscrowd\"] = iscrowd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f17af3d7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-20T20:59:46.949341Z",
          "start_time": "2024-05-20T20:59:39.553984Z"
        },
        "id": "f17af3d7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "# First we need to collect the image associated with the index\n",
        "img = read_image(\"Data/KalAndJinseIdentifying/Images/PXL_20220907_111402520.jpg\")\n",
        "\n",
        "# Next we need to get the associated mask png\n",
        "mask = read_image(\"Data/KalAndJinseIdentifying/SegmentationClass/PXL_20220907_111402520.png\")\n",
        "resized_transform = transforms.Resize((438, 567), interpolation=InterpolationMode.NEAREST)\n",
        "img = resized_transform(img)\n",
        "mask = resized_transform(mask)\n",
        "\n",
        "for h in range(mask.shape[1]):\n",
        "    for w in range(mask.shape[2]):\n",
        "        print(mask[:, h, w])\n",
        "\n",
        "kal_mask = (mask[0, :] == 128)\n",
        "jinse_mask = (mask[1, :] == 128)\n",
        "mask[0, kal_mask] = 1\n",
        "mask[0, jinse_mask] = 2\n",
        "mask = mask[0:1]\n",
        "\n",
        "# Collect all the different unique elements labled in the mask. (In this set each person)\n",
        "obj_ids = torch.unique(mask)\n",
        "\n",
        "# Remove the first element as it is background labeling\n",
        "obj_ids = obj_ids[1:]\n",
        "num_objs = len(obj_ids)\n",
        "print(obj_ids)\n",
        "print(mask.shape)\n",
        "\n",
        "# Making binary mask of objects found.\n",
        "# [:, None, None] is needed to reshape the obj_ids tensor which is a 1D tensor containing the different colors\n",
        "# So that an element wise comparison against the pixels in the mask can be done. given Trues where pixels\n",
        "# part of an object and false elsewhere. Then translating that to 1 and 0s instead of booleans.\n",
        "masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "print(masks.shape)\n",
        "\n",
        "# get boxes of that bound the objects\n",
        "boxes = masks_to_boxes(masks)\n",
        "\n",
        "labels = []\n",
        "if 1 in obj_ids:\n",
        "    labels.append(\"Kal\")\n",
        "if 2 in obj_ids:\n",
        "    labels.append(\"Jinse\")\n",
        "\n",
        "output_image = draw_bounding_boxes(img, boxes, labels, colors=[\"green\", \"red\"], width=5)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(output_image.permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6df08c12",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-19T00:04:18.072884Z",
          "start_time": "2024-05-19T00:04:18.045567Z"
        },
        "id": "6df08c12",
        "outputId": "90fdc458-2460-47c0-98d5-d9ebc3e7e453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 4], dtype=torch.uint8)\n",
            "torch.Size([1, 438, 567])\n",
            "torch.Size([4, 438, 567])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "img = read_image(os.path.join('Data/PennFudanPed', 'PNGImages', 'FudanPed00046.png'))\n",
        "mask = read_image(os.path.join('Data/PennFudanPed', 'PedMasks', 'FudanPed00046_mask.png'))\n",
        "\n",
        "# Collect all the different unique elements labled in the mask. (In this set each person)\n",
        "obj_ids = torch.unique(mask)\n",
        "\n",
        "# Remove the first element as it is background labeling\n",
        "obj_ids = obj_ids[1:]\n",
        "num_objs = len(obj_ids)\n",
        "print(obj_ids)\n",
        "print(mask.shape)\n",
        "\n",
        "# Making binary mask of objects found.\n",
        "# [:, None, None] is needed to reshape the obj_ids tensor which is a 1D tensor containing the different colors\n",
        "# So that an element wise comparison against the pixels in the mask can be done. given Trues where pixels\n",
        "# part of an object and false elsewhere. Then translating that to 1 and 0s instead of booleans.\n",
        "masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "print(masks.shape)\n",
        "\n",
        "# get boxes of that bound the objects\n",
        "boxes = masks_to_boxes(masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b66cce0f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-19T00:04:18.078028Z",
          "start_time": "2024-05-19T00:04:18.073574Z"
        },
        "id": "b66cce0f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "class KalAndJinseDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transforms=None):\n",
        "        # Collect image file names to sorted list\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.image_file_list = sorted(os.listdir(image_dir))\n",
        "        self.mask_file_list = sorted(os.listdir(mask_dir))\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # First we need to collect the image associated with the index\n",
        "        img = read_image(os.path.join(self.image_dir, self.image_file_list[idx]))\n",
        "\n",
        "        # Next we need to get the associated mask png\n",
        "        mask = read_image(os.path.join(self.mask_dir, self.mask_file_list[idx]))\n",
        "\n",
        "        # Convert the pixels to single representative identifiers.\n",
        "        kal_mask = (mask[0, :] == 128)\n",
        "        jinse_mask = (mask[1, :] == 128)\n",
        "        mask[0, kal_mask] = 1\n",
        "        mask[0, jinse_mask] = 2\n",
        "        # Convert to a single dimention for pixel i.e. [1, H, W]\n",
        "        mask = mask[0:1]\n",
        "\n",
        "        # Collect all the different unique elements labled in the mask. (In this set each person)\n",
        "        obj_ids = torch.unique(mask)\n",
        "\n",
        "        # Remove the first element as it is background labeling\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # Making binary mask of objects found.\n",
        "        # [:, None, None] is needed to reshape the obj_ids tensor which is a 1D tensor containing the different colors\n",
        "        # So that an element wise comparison against the pixels in the mask can be done. given Trues where pixels\n",
        "        # part of an object and false elsewhere. Then translating that to 1 and 0s instead of booleans.\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get boxes of that bound the objects\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "#         labels = []\n",
        "#         if 1 in obj_ids:\n",
        "#             labels.append(\"Kal\")\n",
        "#         if 2 in obj_ids:\n",
        "#             labels.append(\"Jinse\")\n",
        "\n",
        "        image_id = idx\n",
        "\n",
        "        # Boxes are in this format:\n",
        "        # Column 0: x-coordinate of the top-left corner\n",
        "        # Column 1: y-coordinate of the top-left corner\n",
        "        # Column 2: x-coordinate of the bottom-right corner\n",
        "        # Column 3: y-coordinate of the bottom-right corner\n",
        "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
        "\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Make image a TVTensor\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size = F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = obj_ids.to(dtype=torch.int64)\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b38ba454",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-19T00:04:18.083500Z",
          "start_time": "2024-05-19T00:04:18.078746Z"
        },
        "id": "b38ba454"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # load pre-trained model\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # get input features for bounding box\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # replace with our predictor\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # Get segmentation predictor input features\n",
        "    in_features = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    # Common hidden layer value\n",
        "    hidden_layer = 256\n",
        "\n",
        "    # replace\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features, hidden_layer, num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd78c412",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-19T00:04:18.085885Z",
          "start_time": "2024-05-19T00:04:18.084193Z"
        },
        "id": "dd78c412"
      },
      "outputs": [],
      "source": [
        "# Provide transformations\n",
        "\n",
        "from torchvision.transforms import v2 as T\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "#     transforms.append(T.Resize((438, 567), interpolation=InterpolationMode.NEAREST))\n",
        "    return T.Compose(transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5facbc72",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-19T00:04:24.785122Z",
          "start_time": "2024-05-19T00:04:18.086529Z"
        },
        "id": "5facbc72",
        "outputId": "11246ad9-c43f-4fbf-ac56-384ebd2fc71e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss_classifier': tensor(0.3232, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0798, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0037, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0014, grad_fn=<DivBackward0>)}\n",
            "{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)}\n"
          ]
        }
      ],
      "source": [
        "# Test functionality before actual training and evaluating\n",
        "\n",
        "import utils\n",
        "\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "dataset = KalAndJinseDataset('Data/KalAndJinseIdentifying/Images', 'Data/KalAndJinseIdentifying/SegmentationClass', get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "# For Training\n",
        "images, targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images, targets)  # Returns losses and detections\n",
        "print(output)\n",
        "\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)  # Returns predictions\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7307cc51",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-19T00:37:39.936428Z",
          "start_time": "2024-05-19T00:14:48.378402Z"
        },
        "id": "7307cc51"
      },
      "outputs": [],
      "source": [
        "# Training & dataset in train and test set (Save last 15 for test set)\n",
        "indices = torValidation\n",
        "\n",
        "from engine import train_one_epoch, evaluate\n",
        "\n",
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has 3 classes only - background, kal, jinse\n",
        "num_classes = 3\n",
        "# use our dataset and defined transformations\n",
        "dataset = KalAndJinseDataset('Data/KalAndJinseIdentifying/Images', 'Data/KalAndJinseIdentifying/SegmentationClass', get_transform(train=True))\n",
        "dataset_test = KalAndJinseDataset('Data/KalAndJinseIdentifying/Images', 'Data/KalAndJinseIdentifying/SegmentationClass', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set (Save last 15 for test set)\n",
        "indices = tordataset in train and test set (Save last 15 for test set)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-15])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-15:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "# get the model using our helper function\n",
        "# model = get_model_instance_segmentation(num_classes)\n",
        "\n",
        "# # move model to the right device\n",
        "# model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "# let's train it just for 10 epochs\n",
        "num_epochs = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "print(\"Training Completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b45f92e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-18T23:42:22.386628Z",
          "start_time": "2024-05-18T23:42:21.130540Z"
        },
        "scrolled": true,
        "id": "4b45f92e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
        "\n",
        "\n",
        "image = read_image(\"Data/KalAndJinseIdentifying/Images/PXL_20220924_193720820.jpg\")\n",
        "eval_transform = get_transform(train=False)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x = eval_transform(image)\n",
        "    # convert RGBA -> RGB and move to device\n",
        "    x = x[:3, ...].to(device)\n",
        "    predictions = model([x, ])\n",
        "    pred = predictions[0]\n",
        "good_score_mask = pred[\"scores\"] > 0.5\n",
        "kal_mask = pred[\"labels\"] == 1\n",
        "jinse_mask = pred[\"labels\"] == 2\n",
        "\n",
        "kal_mask = good_score_mask & kal_mask\n",
        "jinse_mask = good_score_mask & jinse_mask\n",
        "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
        "image = image[:3, ...]\n",
        "# kal_mask = pred[\"labels\"][good_scores_mask] == 1\n",
        "# jinse_mask = pred[\"labels\"][good_scores_mask] == 2\n",
        "kal_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"][kal_mask], pred[\"scores\"][kal_mask])]\n",
        "kal_boxes = pred[\"boxes\"][kal_mask].long()\n",
        "output_image = draw_bounding_boxes(image, kal_boxes, kal_labels, colors=\"yellow\")\n",
        "\n",
        "jinse_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"][jinse_mask], pred[\"scores\"][jinse_mask])]\n",
        "jinse_boxes = pred[\"boxes\"][jinse_mask].long()\n",
        "output_image = draw_bounding_boxes(output_image, jinse_boxes, jinse_labels, colors=\"red\")\n",
        "\n",
        "kal_masks = (pred[\"masks\"][kal_mask] > 0.5).squeeze(1)\n",
        "output_image = draw_segmentation_masks(output_image, kal_masks, alpha=0.5, colors=\"yellow\")\n",
        "\n",
        "jinse_masks = (pred[\"masks\"][jinse_mask] > 0.5).squeeze(1)\n",
        "output_image = draw_segmentation_masks(output_image, jinse_masks, alpha=0.5, colors=\"red\")\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(output_image.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42769e17",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-19T00:41:38.328853Z",
          "start_time": "2024-05-19T00:41:38.088725Z"
        },
        "id": "42769e17"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), \"SavedModels/KalAndJinseIdentifier/KalAndJinseIdentifierModel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1086c71e",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-19T04:00:05.540Z"
        },
        "id": "1086c71e"
      },
      "outputs": [],
      "source": [
        "# Stream Webcamera now to identify in real time. Before putting it on RC car.\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_model_instance_segmentation(3)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load(\"SavedModels/KalAndJinseIdentifier/KalAndJinseIdentifierModel\"))\n",
        "\n",
        "video_capture = cv2.VideoCapture(0)\n",
        "\n",
        "if not video_capture.isOpened():\n",
        "    raise IOError(\"Cannot open webcam\")\n",
        "\n",
        "while True:\n",
        "    ret, frame = video_capture.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    ## Process image with model\n",
        "    # Convert from BGR (OpenCV format) to RGB (PyTorch format)\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # Convert from [H, W, C] to [C, H, W] tensor\n",
        "    frame_tensor = torch.from_numpy(frame).permute(2, 0, 1)\n",
        "    image = frame_tensor\n",
        "\n",
        "    eval_transform = get_transform(train=False)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = eval_transform(image)\n",
        "        # convert RGBA -> RGB and move to device\n",
        "        x = x[:3, ...].to(device)\n",
        "        predictions = model([x, ])\n",
        "        pred = predictions[0]\n",
        "    good_score_mask = pred[\"scores\"] > 0.8\n",
        "    kal_mask = pred[\"labels\"] == 1\n",
        "    jinse_mask = pred[\"labels\"] == 2\n",
        "\n",
        "    kal_mask = good_score_mask & kal_mask\n",
        "    jinse_mask = good_score_mask & jinse_mask\n",
        "    image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
        "    image = image[:3, ...]\n",
        "    # kal_mask = pred[\"labels\"][good_scores_mask] == 1\n",
        "    # jinse_mask = pred[\"labels\"][good_scores_mask] == 2\n",
        "    kal_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"][kal_mask], pred[\"scores\"][kal_mask])]\n",
        "    kal_boxes = pred[\"boxes\"][kal_mask].long()\n",
        "    output_image = draw_bounding_boxes(image, kal_boxes, kal_labels, colors=\"yellow\")\n",
        "\n",
        "    jinse_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"][jinse_mask], pred[\"scores\"][jinse_mask])]\n",
        "    jinse_boxes = pred[\"boxes\"][jinse_mask].long()\n",
        "    output_image = draw_bounding_boxes(output_image, jinse_boxes, jinse_labels, colors=\"red\")\n",
        "\n",
        "    kal_masks = (pred[\"masks\"][kal_mask] > 0.5).squeeze(1)\n",
        "    output_image = draw_segmentation_masks(output_image, kal_masks, alpha=0.5, colors=\"yellow\")\n",
        "\n",
        "    jinse_masks = (pred[\"masks\"][jinse_mask] > 0.5).squeeze(1)\n",
        "    output_image = draw_segmentation_masks(output_image, jinse_masks, alpha=0.5, colors=\"red\")\n",
        "\n",
        "    # Convert output_image back to cv2 format\n",
        "    output_frame = output_image.permute(1, 2, 0).numpy()\n",
        "    output_frame = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2.imshow('Detect Jinse & Kal', output_frame)\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f03ac73",
      "metadata": {
        "id": "2f03ac73"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}