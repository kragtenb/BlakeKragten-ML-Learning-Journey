{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf32931",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:56:42.407501Z",
     "start_time": "2024-05-21T03:56:42.396308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x771ebd4ff400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb3fc246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T04:00:22.953395Z",
     "start_time": "2024-05-21T04:00:22.948037Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "class DrivingDataset(torch.utils.data.Dataset):\n",
    "    classes = [\"Background\", \"Drivable\", \"Wall\", \"Obstacle\"]\n",
    "    \n",
    "    def __init__(self, image_dir, mask_dir, transforms=None):\n",
    "        # Collect image file names to sorted list\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_file_list = sorted(os.listdir(image_dir))\n",
    "        self.mask_file_list = sorted(os.listdir(mask_dir))\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # First we need to collect the image associated with the index\n",
    "        img = read_image(os.path.join(self.image_dir, self.image_file_list[idx]))\n",
    "        \n",
    "        # Next we need to get the associated mask png\n",
    "        mask = read_image(os.path.join(self.mask_dir, self.mask_file_list[idx]))\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "            mask = self.transforms(mask)\n",
    "        \n",
    "        # Convert the pixels to single representative identifiers. \n",
    "#         for h in range(mask.shape[1]):\n",
    "#             for w in range(mask.shape[2]):\n",
    "#                 if (mask[:, h, w] == [128, 0, 0]):\n",
    "#                     # Drivable\n",
    "#                     mask[0, h, w] = 1\n",
    "#                 elif (mask[:, h, w] == [0, 128, 0]):\n",
    "#                     # Wall\n",
    "#                     mask[0, h, w] = 2\n",
    "#                 elif (mask[:, h, w] == [128, 128, 0]):\n",
    "#                     # Obstacle\n",
    "#                     mask[0, h, w] = 3\n",
    "#                 else:\n",
    "#                     # Background\n",
    "#                     mask[0, h, w] = 0\n",
    "        print(1)\n",
    "        mask[:3, :, :][mask[:3, :, :] == [128, 0, 0]] = 1\n",
    "        mask[:3, :, :][mask[:3, :, :] == [0, 128, 0]] = 2\n",
    "        mask[:3, :, :][mask[:3, :, :] == [128, 128, 0]] = 3\n",
    "        print(2)\n",
    "    \n",
    "        # Convert to a single dimention for pixel i.e. [1, H, W]\n",
    "        mask = mask[0:1]\n",
    "\n",
    "        # Collect all the different unique elements labled in the mask. (In this set each person)\n",
    "        obj_ids = torch.unique(mask)\n",
    "\n",
    "        # Remove the first element as it is background labeling\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # Making binary mask of objects found.\n",
    "        # [:, None, None] is needed to reshape the obj_ids tensor which is a 1D tensor containing the different colors\n",
    "        # So that an element wise comparison against the pixels in the mask can be done. given Trues where pixels\n",
    "        # part of an object and false elsewhere. Then translating that to 1 and 0s instead of booleans.\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get boxes of that bound the objects\n",
    "        boxes = masks_to_boxes(masks)\n",
    "            \n",
    "        image_id = idx\n",
    "        \n",
    "        # Boxes are in this format:\n",
    "        # Column 0: x-coordinate of the top-left corner\n",
    "        # Column 1: y-coordinate of the top-left corner\n",
    "        # Column 2: x-coordinate of the bottom-right corner\n",
    "        # Column 3: y-coordinate of the bottom-right corner\n",
    "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        # Make image a TVTensor\n",
    "        img = tv_tensors.Image(img)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size = F.get_size(img))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = obj_ids.to(dtype=torch.int64)\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e785588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T04:00:23.970340Z",
     "start_time": "2024-05-21T04:00:23.966545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here we will show the data augmentations and normalizations that will improve training\n",
    "# On validation data we will only do normalizations.\n",
    "# We need to pull the datasets as well and make the dataloaders based on the pre-made datasets\n",
    "\n",
    "# Dict that will have the transforms for training data alongside validation.\n",
    "data_transforms = {\n",
    "    'train': T.Compose([\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.Resize(800, max_size=1333),\n",
    "        # Horizontally flip the given image randomly with a given probability. Default is .5\n",
    "        T.RandomHorizontalFlip(),\n",
    "        # Makes output a tensor\n",
    "        T.ToPureTensor(),\n",
    "        # First array is Mean subtraction so it  subtracts the mean value ([0.485, 0.456, 0.406]) from each\n",
    "        # color channel (red, green, blue) of the image. This shifts the pixel values closer to zero.\n",
    "        # Second array is division by standard deviation it divides each color by the values here. \n",
    "        # This will scale the pixel values to have a similar range across different channels\n",
    "        # These numbers were chosen specifically because these are the calculated average mean\n",
    "        # and standard deviation\n",
    "        # across the entire ImageNet dataset.\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': T.Compose([\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.Resize(800, max_size=1333),\n",
    "        T.ToPureTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_dir = 'Data/SelfDrivingRC/Images'\n",
    "mask_dir = 'Data/SelfDrivingRC/SegmentationClass'\n",
    "# Dict of train data and val data, keyed by 'train' & 'val' respectfully\n",
    "image_datasets = {x: DrivingDataset(os.path.join(image_dir, x),\n",
    "                                    os.path.join(mask_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a94e5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T04:00:24.721016Z",
     "start_time": "2024-05-21T04:00:24.719258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Background', 'Drivable', 'Wall', 'Obstacle']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a284f698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T04:00:25.468988Z",
     "start_time": "2024-05-21T04:00:25.297635Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/blake/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/blake/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/blake/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_35835/2566144181.py\", line 36, in __getitem__\n    mask.to(device)\n  File \"/home/blake/.local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 279, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mpause(\u001b[38;5;241m0.001\u001b[39m)  \u001b[38;5;66;03m# pause a bit so that plots are updated\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Get a batch of training data\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m inputs, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m imshow(inputs)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Make a grid from batch\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# out = torchvision.utils.make_grid(inputs)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# imshow(out)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/blake/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/blake/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/blake/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_35835/2566144181.py\", line 36, in __getitem__\n    mask.to(device)\n  File \"/home/blake/.local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 279, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, target = next(iter(dataloaders['train']))\n",
    "\n",
    "imshow(inputs)\n",
    "\n",
    "# Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out)\n",
    "\n",
    "# # Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(target['masks'])\n",
    "# imshow(out, title=[y for y in [class_names[x]] for x in target['masks']])\n",
    "\n",
    "# imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36653583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
