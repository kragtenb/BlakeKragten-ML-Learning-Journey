{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d31138-1fdf-4553-a3d3-a1f8d4af627c",
   "metadata": {},
   "source": [
    "# This document will implement the Chapter 10 models and exercises but will use pytorch instead of tensorflow and keras\n",
    "## Fashion NIST Simple NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b16cd86-1a9e-46ff-892b-e758799b1e94",
   "metadata": {},
   "source": [
    "In pytorch, instead of creating a model like in tensor flow that is stand alone from APIs. It looks like it is more object oriented and uses a class structure instead. \n",
    "\n",
    "First step will be to get the fashion data into a format that can be used with the model itself. This requires the creation of Dataset & Dataloader.\n",
    "\n",
    "Dataset is responsible for accessing and processing single instances of the data. \n",
    "\n",
    "While a Dataloader pulls instaces of the data from the Dataset, collects them in batches and returns them for use in training.\n",
    "\n",
    "Part of the dataset creation, we will normalize the images to be zero centered as these are more efficient to work with causing training loop to be shorter in runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "33711b3f-9087-43ce-a8ca-464a03242ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Dataset and DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cd1075e4-476e-4506-b848-83677c73a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tensor board support for better visualization of the model we are creating.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0226d5-f712-49d8-b580-6aeb03aed7ae",
   "metadata": {},
   "source": [
    "With pytorch we can use GPU for faster training speed. Figure out if we can with this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e94823ca-e350-4d3e-85ff-97ac16fb3a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "028db700-4bec-473e-bc93-0a4a625da18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "investigate_mnist_data_shape = torchvision.datasets.FashionMNIST('Data/FashionMNIST', train=True, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bfaaf874-2541-4177-a22b-c8fcee712d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate the size of the data\n",
    "investigate_mnist_data_shape.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3e8ce-f8b5-42eb-ad58-f7330ca5369a",
   "metadata": {},
   "source": [
    "The data is 60_000 images of the shape (28, 28). Thus for the normalized transform we need to have to dimensions for the (mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3557774f-4978-446c-89b7-a53ebf749cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a transformer function. This will convert the dataset to a tensor and\n",
    "# apply the normalization to make it 0 centered and standard deviation around 0.5\n",
    "\n",
    "mean = 0.5\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize needs a mean for each channel. Being that this is a grayscale we only have one channel\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ]    \n",
    ")\n",
    "\n",
    "training_set = torchvision.datasets.FashionMNIST('Data/FashionMNIST', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('Data/FashionMNIST', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b476b1-bc0a-41fb-8f73-65ba378821f8",
   "metadata": {},
   "source": [
    "We now have our datasets. As the data was already in torchvision we don't need to explicitly create the classes themselves yet.\n",
    "\n",
    "Now we need to create a DataLoader that will take in this DataSet for both training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7cc7820-a534-4308-b286-504253869f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67826a7a-b680-4ea4-b16c-daf5f7cd349a",
   "metadata": {},
   "source": [
    "Generate our class labels as the FashionMNIST data set does not have any labels, just numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "361ee6d2-6838-4246-8d81-b0b87ff23c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbac035-df62-4c13-aca4-df64cbb329cd",
   "metadata": {},
   "source": [
    "To make sure the data is looking like how we should expect we should visualize the data we received using MatPlotLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d9a8976d-dc75-45f0-a95a-7f7ee6bfb9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trouser Bag Ankle Boot Trouser\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_image(img):\n",
    "    img = img.mean(dim=0)\n",
    "    # Remove normalization\n",
    "    img = img / 2 + 0.5\n",
    "    # Convert from tensor to np.array\n",
    "    img_np = img.numpy()\n",
    "    plt.imshow(img_np, cmap='grey')\n",
    "\n",
    "# Data iterator of the data loader\n",
    "data_iter = iter(training_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Create a grid of the images to be displayed\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "display_image(img_grid)\n",
    "\n",
    "print(' '.join(classes[label] for label in labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be349db1-dc48-4898-8c58-69bba438baff",
   "metadata": {},
   "source": [
    "The labels and the data matches. Sanity Check complete. Good to move forware to setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1c0260c-459d-425b-b1a1-81ecb8333350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neural network and activation function libraries\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ba02f5c-8b0e-4579-9c7e-951fa0811be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch models are created by inheriting nn.Module\n",
    "class FashionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(28*28, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.sequential(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a08200bc-8a32-4bf8-89d7-7b87c010bd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionClassifier(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (sequential): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "    (5): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FashionClassifier()\n",
    "# model = FashionClassifier().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4867f1-0563-4390-a10b-48cd74759099",
   "metadata": {},
   "source": [
    "Woot, model looks fairly similar to the model we created in the TensorFlow Document\n",
    "\n",
    "Now it is time to train the model. In pytorch the steps are more drawn out. Instead of doing fit like in tensorflow we need to list each step in the training loop and make the loop as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eb29eafd-8843-45ca-91c4-9092ee5bb8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Get Optimizer, Using Parameters from tensorflow example\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6fdca-0e7f-41f9-abfe-ed5908a92f14",
   "metadata": {},
   "source": [
    "For the training loop the general flow is as follows:\n",
    "1. Get a batch of data from Dataloader\n",
    "2. Zero optimizer's gradient\n",
    "3. Query model for predictions\n",
    "4. Calculate loss between predictions and labels\n",
    "5. Calculate backward gradients over the learning weights\n",
    "6. Tell the optimizer to perform 1 step to adjust the weights based on the backwards gradients\n",
    "7. Report on loss and average loss every to often to see it is working (1_000 batches) and not too often to cause too much non-gpu time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e36fa153-358d-4e84-ac60-7870953fcaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One epoch training function\n",
    "def one_epoch_training(epoch_index, tb_writer):\n",
    "    '''\n",
    "        Parameters:\n",
    "            epoch_index: Specifies the current epoch being trained\n",
    "            tb_writer: Specifies the TensorBoard Writer for visualization\n",
    "\n",
    "        Returns:\n",
    "            Final loss value for epoch\n",
    "    '''\n",
    "    running_loss = 0.\n",
    "    final_loss = 0.\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Loop through all batches of images \n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        # inputs = inputs.to(device)\n",
    "        # labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(inputs)\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Update the running total of correct predictions and samples\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        if i % 1000 == 999:\n",
    "            final_loss = running_loss / 1000 # loss per batch\n",
    "            accuracy = 100 * total_correct / total_samples\n",
    "            print(f' batch {i+1} lost: {final_loss} accuracy: {accuracy}')\n",
    "            images_trained = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train/accuracy', final_loss, images_trained, accuracy)\n",
    "            running_loss = 0.\n",
    "\n",
    "    accuracy = 100 * total_correct / total_samples\n",
    "    return final_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b1eeae5d-c787-49bb-be32-4704674c1bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      " batch 1000 lost: 2.277395451068878 accuracy: 30.275\n",
      " batch 2000 lost: 2.0407505815029143 accuracy: 38.3625\n",
      " batch 3000 lost: 1.8530051285028457 accuracy: 47.49166666666667\n",
      " batch 4000 lost: 1.7587774111032486 accuracy: 53.9625\n",
      " batch 5000 lost: 1.7322047610282898 accuracy: 58.05\n",
      " batch 6000 lost: 1.7200182992219926 accuracy: 60.97083333333333\n",
      " batch 7000 lost: 1.6918615484237671 accuracy: 63.457142857142856\n",
      " batch 8000 lost: 1.7067465267181396 accuracy: 65.075\n",
      " batch 9000 lost: 1.700732468366623 accuracy: 66.40277777777777\n",
      " batch 10000 lost: 1.6908083832263947 accuracy: 67.5275\n",
      " batch 11000 lost: 1.6785322519540786 accuracy: 68.55454545454545\n",
      " batch 12000 lost: 1.6948302332162857 accuracy: 69.29791666666667\n",
      " batch 13000 lost: 1.6750744414329528 accuracy: 70.04615384615384\n",
      " batch 14000 lost: 1.6784543936252594 accuracy: 70.66607142857143\n",
      " batch 15000 lost: 1.6703153883218764 accuracy: 71.275\n",
      "LOSS train 1.6703153883218764, valid 1.6768196821212769\n",
      "ACCURACY train 71.275, valid 78.94\n",
      "EPOCH 2:\n",
      " batch 1000 lost: 1.675756765127182 accuracy: 78.9\n",
      " batch 2000 lost: 1.6775767891407012 accuracy: 78.85\n",
      " batch 3000 lost: 1.6650913932323457 accuracy: 79.21666666666667\n",
      " batch 4000 lost: 1.6645922685861587 accuracy: 79.4\n",
      " batch 5000 lost: 1.6658302519321442 accuracy: 79.5\n",
      " batch 6000 lost: 1.664758749961853 accuracy: 79.5625\n",
      " batch 7000 lost: 1.6662189326286316 accuracy: 79.60357142857143\n",
      " batch 8000 lost: 1.6599441854953765 accuracy: 79.675\n",
      " batch 9000 lost: 1.6584323740005493 accuracy: 79.79722222222222\n",
      " batch 10000 lost: 1.6621740552186965 accuracy: 79.8125\n",
      " batch 11000 lost: 1.6618764508962631 accuracy: 79.85\n",
      " batch 12000 lost: 1.6578522436618806 accuracy: 79.89791666666666\n",
      " batch 13000 lost: 1.6703198840618134 accuracy: 79.86538461538461\n",
      " batch 14000 lost: 1.666044427394867 accuracy: 79.8625\n",
      " batch 15000 lost: 1.6571136239767075 accuracy: 79.93166666666667\n",
      "LOSS train 1.6571136239767075, valid 1.6654400825500488\n",
      "ACCURACY train 79.93166666666667, valid 79.71\n",
      "EPOCH 3:\n",
      " batch 1000 lost: 1.6429440373182296 accuracy: 82.325\n",
      " batch 2000 lost: 1.6650521460771561 accuracy: 81.125\n",
      " batch 3000 lost: 1.657544993519783 accuracy: 80.95\n",
      " batch 4000 lost: 1.6525086600780488 accuracy: 81.0375\n",
      " batch 5000 lost: 1.6555867687463761 accuracy: 80.955\n",
      " batch 6000 lost: 1.660767110824585 accuracy: 80.82916666666667\n",
      " batch 7000 lost: 1.6481438267230988 accuracy: 80.93214285714286\n",
      " batch 8000 lost: 1.652688866019249 accuracy: 80.94375\n",
      " batch 9000 lost: 1.662704096198082 accuracy: 80.84166666666667\n",
      " batch 10000 lost: 1.6523897989988328 accuracy: 80.87\n",
      " batch 11000 lost: 1.6572926051616668 accuracy: 80.83636363636364\n",
      " batch 12000 lost: 1.6457262127399443 accuracy: 80.89791666666666\n",
      " batch 13000 lost: 1.649883883357048 accuracy: 80.925\n",
      " batch 14000 lost: 1.6532697162628174 accuracy: 80.93214285714286\n",
      " batch 15000 lost: 1.6514287885427474 accuracy: 80.94333333333333\n",
      "LOSS train 1.6514287885427474, valid 1.6547417640686035\n",
      "ACCURACY train 80.94333333333333, valid 80.8\n",
      "EPOCH 4:\n",
      " batch 1000 lost: 1.6470935767889023 accuracy: 81.425\n",
      " batch 2000 lost: 1.6434706344604493 accuracy: 81.65\n",
      " batch 3000 lost: 1.6518481273651122 accuracy: 81.40833333333333\n",
      " batch 4000 lost: 1.660929859638214 accuracy: 81.125\n",
      " batch 5000 lost: 1.6439409071207047 accuracy: 81.265\n",
      " batch 6000 lost: 1.6541979180574418 accuracy: 81.17083333333333\n",
      " batch 7000 lost: 1.6520434631109238 accuracy: 81.16071428571429\n",
      " batch 8000 lost: 1.648406679868698 accuracy: 81.159375\n",
      " batch 9000 lost: 1.6433686757087707 accuracy: 81.24444444444444\n",
      " batch 10000 lost: 1.645025980591774 accuracy: 81.295\n",
      " batch 11000 lost: 1.653291208744049 accuracy: 81.25227272727273\n",
      " batch 12000 lost: 1.6421977059841155 accuracy: 81.325\n",
      " batch 13000 lost: 1.6477269071340561 accuracy: 81.325\n",
      " batch 14000 lost: 1.6418545768260955 accuracy: 81.38035714285714\n",
      " batch 15000 lost: 1.6422040051221847 accuracy: 81.425\n",
      "LOSS train 1.6422040051221847, valid 1.6518781185150146\n",
      "ACCURACY train 81.425, valid 80.89\n",
      "EPOCH 5:\n",
      " batch 1000 lost: 1.6477631604671479 accuracy: 81.525\n",
      " batch 2000 lost: 1.6462952489852904 accuracy: 81.5\n",
      " batch 3000 lost: 1.6528243535757066 accuracy: 81.26666666666667\n",
      " batch 4000 lost: 1.6357034738063811 accuracy: 81.60625\n",
      " batch 5000 lost: 1.6472260683774949 accuracy: 81.56\n",
      " batch 6000 lost: 1.6345547815561294 accuracy: 81.7375\n",
      " batch 7000 lost: 1.647271262049675 accuracy: 81.7\n",
      " batch 8000 lost: 1.6442601214647292 accuracy: 81.725\n",
      " batch 9000 lost: 1.640940199971199 accuracy: 81.76944444444445\n",
      " batch 10000 lost: 1.6533648672103882 accuracy: 81.68\n",
      " batch 11000 lost: 1.6362480173110963 accuracy: 81.76818181818182\n",
      " batch 12000 lost: 1.6310823723077774 accuracy: 81.88541666666667\n",
      " batch 13000 lost: 1.6372477427721024 accuracy: 81.92307692307692\n",
      " batch 14000 lost: 1.6461069527864456 accuracy: 81.90357142857142\n",
      " batch 15000 lost: 1.6446713534593582 accuracy: 81.89166666666667\n",
      "LOSS train 1.6446713534593582, valid 1.6566230058670044\n",
      "ACCURACY train 81.89166666666667, valid 80.44\n",
      "EPOCH 6:\n",
      " batch 1000 lost: 1.6337888191938401 accuracy: 82.85\n",
      " batch 2000 lost: 1.6424413553476334 accuracy: 82.4125\n",
      " batch 3000 lost: 1.6406056514978409 accuracy: 82.33333333333333\n",
      " batch 4000 lost: 1.647440113544464 accuracy: 82.10625\n",
      " batch 5000 lost: 1.6369568064212798 accuracy: 82.19\n",
      " batch 6000 lost: 1.636833045363426 accuracy: 82.25\n",
      " batch 7000 lost: 1.6377232030630111 accuracy: 82.275\n",
      " batch 8000 lost: 1.6453186790943146 accuracy: 82.196875\n",
      " batch 9000 lost: 1.6435352598428725 accuracy: 82.14722222222223\n",
      " batch 10000 lost: 1.6392952871322632 accuracy: 82.135\n",
      " batch 11000 lost: 1.6380734388828277 accuracy: 82.16363636363636\n",
      " batch 12000 lost: 1.6384669960737228 accuracy: 82.18541666666667\n",
      " batch 13000 lost: 1.6300564261674881 accuracy: 82.27884615384616\n",
      " batch 14000 lost: 1.6344732627868652 accuracy: 82.31964285714285\n",
      " batch 15000 lost: 1.6463179038763047 accuracy: 82.265\n",
      "LOSS train 1.6463179038763047, valid 1.6636179685592651\n",
      "ACCURACY train 82.265, valid 79.88\n",
      "EPOCH 7:\n",
      " batch 1000 lost: 1.633981771349907 accuracy: 82.9\n",
      " batch 2000 lost: 1.6350534496307374 accuracy: 82.8375\n",
      " batch 3000 lost: 1.6387084094285964 accuracy: 82.66666666666667\n",
      " batch 4000 lost: 1.6450456311702728 accuracy: 82.4125\n",
      " batch 5000 lost: 1.6374769934415818 accuracy: 82.425\n",
      " batch 6000 lost: 1.6436882908344268 accuracy: 82.32083333333334\n",
      " batch 7000 lost: 1.6388083082437515 accuracy: 82.34642857142858\n",
      " batch 8000 lost: 1.6428668746948243 accuracy: 82.28125\n",
      " batch 9000 lost: 1.6278119453191757 accuracy: 82.41111111111111\n",
      " batch 10000 lost: 1.6375257484912873 accuracy: 82.4175\n",
      " batch 11000 lost: 1.632410586953163 accuracy: 82.4659090909091\n",
      " batch 12000 lost: 1.6370453258752824 accuracy: 82.46666666666667\n",
      " batch 13000 lost: 1.6323199172019958 accuracy: 82.49038461538461\n",
      " batch 14000 lost: 1.6298211646080016 accuracy: 82.55\n",
      " batch 15000 lost: 1.6379209730625153 accuracy: 82.53333333333333\n",
      "LOSS train 1.6379209730625153, valid 1.6458996534347534\n",
      "ACCURACY train 82.53333333333333, valid 81.54\n",
      "EPOCH 8:\n",
      " batch 1000 lost: 1.6207869338989258 accuracy: 84.225\n",
      " batch 2000 lost: 1.637746340751648 accuracy: 83.3375\n",
      " batch 3000 lost: 1.6388157465457915 accuracy: 82.95\n",
      " batch 4000 lost: 1.6335365080833435 accuracy: 82.89375\n",
      " batch 5000 lost: 1.6436248252391814 accuracy: 82.715\n",
      " batch 6000 lost: 1.6314568490982055 accuracy: 82.77083333333333\n",
      " batch 7000 lost: 1.638070697903633 accuracy: 82.70714285714286\n",
      " batch 8000 lost: 1.6345880942344666 accuracy: 82.715625\n",
      " batch 9000 lost: 1.632942453622818 accuracy: 82.74166666666666\n",
      " batch 10000 lost: 1.637217506289482 accuracy: 82.72\n",
      " batch 11000 lost: 1.633217919111252 accuracy: 82.7159090909091\n",
      " batch 12000 lost: 1.6269396589994432 accuracy: 82.77916666666667\n",
      " batch 13000 lost: 1.6316082317829133 accuracy: 82.79230769230769\n",
      " batch 14000 lost: 1.6432839677333833 accuracy: 82.7125\n",
      " batch 15000 lost: 1.634110307455063 accuracy: 82.725\n",
      "LOSS train 1.634110307455063, valid 1.6414480209350586\n",
      "ACCURACY train 82.725, valid 82.05\n",
      "EPOCH 9:\n",
      " batch 1000 lost: 1.6316605982780457 accuracy: 83.075\n",
      " batch 2000 lost: 1.6264508346319198 accuracy: 83.2625\n",
      " batch 3000 lost: 1.6368478534221649 accuracy: 83.0\n",
      " batch 4000 lost: 1.632924046754837 accuracy: 82.9625\n",
      " batch 5000 lost: 1.6450083869695664 accuracy: 82.675\n",
      " batch 6000 lost: 1.6292531645298005 accuracy: 82.7875\n",
      " batch 7000 lost: 1.6296655889749527 accuracy: 82.84285714285714\n",
      " batch 8000 lost: 1.6295709939002991 accuracy: 82.8875\n",
      " batch 9000 lost: 1.6350022917985916 accuracy: 82.85555555555555\n",
      " batch 10000 lost: 1.6335110456943511 accuracy: 82.835\n",
      " batch 11000 lost: 1.6304613196849822 accuracy: 82.86590909090908\n",
      " batch 12000 lost: 1.6293272651433945 accuracy: 82.90208333333334\n",
      " batch 13000 lost: 1.629710346341133 accuracy: 82.92115384615384\n",
      " batch 14000 lost: 1.6327079309225083 accuracy: 82.91785714285714\n",
      " batch 15000 lost: 1.6330075134038926 accuracy: 82.92666666666666\n",
      "LOSS train 1.6330075134038926, valid 1.6439013481140137\n",
      "ACCURACY train 82.92666666666666, valid 81.73\n",
      "EPOCH 10:\n",
      " batch 1000 lost: 1.6318581166267394 accuracy: 83.025\n",
      " batch 2000 lost: 1.6257438850402832 accuracy: 83.2375\n",
      " batch 3000 lost: 1.6277839186191558 accuracy: 83.26666666666667\n",
      " batch 4000 lost: 1.6254480991363525 accuracy: 83.3375\n",
      " batch 5000 lost: 1.622305055975914 accuracy: 83.445\n",
      " batch 6000 lost: 1.6332832366228103 accuracy: 83.31666666666666\n",
      " batch 7000 lost: 1.6365272591114044 accuracy: 83.18571428571428\n",
      " batch 8000 lost: 1.6241088479757309 accuracy: 83.246875\n",
      " batch 9000 lost: 1.6318286205530166 accuracy: 83.21944444444445\n",
      " batch 10000 lost: 1.6301351411342622 accuracy: 83.1975\n",
      " batch 11000 lost: 1.62666244161129 accuracy: 83.22045454545454\n",
      " batch 12000 lost: 1.631612761259079 accuracy: 83.2125\n",
      " batch 13000 lost: 1.638291092157364 accuracy: 83.14423076923077\n",
      " batch 14000 lost: 1.6317336839437484 accuracy: 83.12857142857143\n",
      " batch 15000 lost: 1.634504543542862 accuracy: 83.10166666666667\n",
      "LOSS train 1.634504543542862, valid 1.6371098756790161\n",
      "ACCURACY train 83.10166666666667, valid 82.38\n",
      "EPOCH 11:\n",
      " batch 1000 lost: 1.6264478927850723 accuracy: 83.5\n",
      " batch 2000 lost: 1.6331567441225052 accuracy: 83.2\n",
      " batch 3000 lost: 1.6362793323993683 accuracy: 83.0\n",
      " batch 4000 lost: 1.6218478132486343 accuracy: 83.23125\n",
      " batch 5000 lost: 1.6244236540794372 accuracy: 83.34\n",
      " batch 6000 lost: 1.6348034896850585 accuracy: 83.23333333333333\n",
      " batch 7000 lost: 1.6347374622821809 accuracy: 83.13571428571429\n",
      " batch 8000 lost: 1.6154288572072983 accuracy: 83.309375\n",
      " batch 9000 lost: 1.630997701048851 accuracy: 83.27222222222223\n",
      " batch 10000 lost: 1.627456740140915 accuracy: 83.295\n",
      " batch 11000 lost: 1.623225370645523 accuracy: 83.35227272727273\n",
      " batch 12000 lost: 1.6275758440494537 accuracy: 83.35\n",
      " batch 13000 lost: 1.6332411580085755 accuracy: 83.3\n",
      " batch 14000 lost: 1.6309960312843323 accuracy: 83.29107142857143\n",
      " batch 15000 lost: 1.6272859582901 accuracy: 83.30666666666667\n",
      "LOSS train 1.6272859582901, valid 1.6430789232254028\n",
      "ACCURACY train 83.30666666666667, valid 81.76\n",
      "EPOCH 12:\n",
      " batch 1000 lost: 1.6284850286245347 accuracy: 83.125\n",
      " batch 2000 lost: 1.6304792031049729 accuracy: 83.1875\n",
      " batch 3000 lost: 1.6382542201280594 accuracy: 82.90833333333333\n",
      " batch 4000 lost: 1.6125139399766921 accuracy: 83.41875\n",
      " batch 5000 lost: 1.6277464369535446 accuracy: 83.415\n",
      " batch 6000 lost: 1.6205717356204987 accuracy: 83.55416666666666\n",
      " batch 7000 lost: 1.623156676888466 accuracy: 83.58571428571429\n",
      " batch 8000 lost: 1.6307903146743774 accuracy: 83.528125\n",
      " batch 9000 lost: 1.6226460031270982 accuracy: 83.55555555555556\n",
      " batch 10000 lost: 1.6265222800970078 accuracy: 83.5325\n",
      " batch 11000 lost: 1.629609286904335 accuracy: 83.50227272727273\n",
      " batch 12000 lost: 1.6346486859321594 accuracy: 83.44375\n",
      " batch 13000 lost: 1.623579630613327 accuracy: 83.48653846153846\n",
      " batch 14000 lost: 1.6211311149597167 accuracy: 83.53214285714286\n",
      " batch 15000 lost: 1.62410277736187 accuracy: 83.54833333333333\n",
      "LOSS train 1.62410277736187, valid 1.6365748643875122\n",
      "ACCURACY train 83.54833333333333, valid 82.57\n",
      "EPOCH 13:\n",
      " batch 1000 lost: 1.620016666173935 accuracy: 84.125\n",
      " batch 2000 lost: 1.6302736320495606 accuracy: 83.6625\n",
      " batch 3000 lost: 1.6166285638809204 accuracy: 83.90833333333333\n",
      " batch 4000 lost: 1.6260019272565842 accuracy: 83.80625\n",
      " batch 5000 lost: 1.6240582348108292 accuracy: 83.785\n",
      " batch 6000 lost: 1.6209947839975356 accuracy: 83.85\n",
      " batch 7000 lost: 1.618025678396225 accuracy: 83.90714285714286\n",
      " batch 8000 lost: 1.621745288848877 accuracy: 83.903125\n",
      " batch 9000 lost: 1.6282675675153733 accuracy: 83.81666666666666\n",
      " batch 10000 lost: 1.6274745804071427 accuracy: 83.7825\n",
      " batch 11000 lost: 1.6194755529165268 accuracy: 83.83863636363637\n",
      " batch 12000 lost: 1.6339252381324767 accuracy: 83.7375\n",
      " batch 13000 lost: 1.6366695631742478 accuracy: 83.66346153846153\n",
      " batch 14000 lost: 1.6269608047008515 accuracy: 83.65178571428571\n",
      " batch 15000 lost: 1.6194628192186356 accuracy: 83.69166666666666\n",
      "LOSS train 1.6194628192186356, valid 1.6356348991394043\n",
      "ACCURACY train 83.69166666666666, valid 82.59\n",
      "EPOCH 14:\n",
      " batch 1000 lost: 1.622382495880127 accuracy: 83.825\n",
      " batch 2000 lost: 1.6295028793811799 accuracy: 83.4875\n",
      " batch 3000 lost: 1.61902423453331 accuracy: 83.80833333333334\n",
      " batch 4000 lost: 1.6318818839788436 accuracy: 83.60625\n",
      " batch 5000 lost: 1.6188732807636261 accuracy: 83.745\n",
      " batch 6000 lost: 1.6295864821672439 accuracy: 83.64583333333333\n",
      " batch 7000 lost: 1.6128472762107848 accuracy: 83.825\n",
      " batch 8000 lost: 1.6305053693056106 accuracy: 83.746875\n",
      " batch 9000 lost: 1.6091480296850205 accuracy: 83.91388888888889\n",
      " batch 10000 lost: 1.6243519097566606 accuracy: 83.885\n",
      " batch 11000 lost: 1.6308073128461837 accuracy: 83.81136363636364\n",
      " batch 12000 lost: 1.6233700169324874 accuracy: 83.82708333333333\n",
      " batch 13000 lost: 1.627230382323265 accuracy: 83.78461538461538\n",
      " batch 14000 lost: 1.6252797033786774 accuracy: 83.78214285714286\n",
      " batch 15000 lost: 1.6249900245666504 accuracy: 83.76833333333333\n",
      "LOSS train 1.6249900245666504, valid 1.636071801185608\n",
      "ACCURACY train 83.76833333333333, valid 82.51\n",
      "EPOCH 15:\n",
      " batch 1000 lost: 1.6170582474470139 accuracy: 84.625\n",
      " batch 2000 lost: 1.6213485713005067 accuracy: 84.3625\n",
      " batch 3000 lost: 1.6230662388801576 accuracy: 84.2\n",
      " batch 4000 lost: 1.6303940533399581 accuracy: 83.95\n",
      " batch 5000 lost: 1.6219348287582398 accuracy: 83.95\n",
      " batch 6000 lost: 1.6254271014928818 accuracy: 83.8875\n",
      " batch 7000 lost: 1.6225047880411148 accuracy: 83.86428571428571\n",
      " batch 8000 lost: 1.6228595168590545 accuracy: 83.878125\n",
      " batch 9000 lost: 1.6170291812419892 accuracy: 83.94444444444444\n",
      " batch 10000 lost: 1.6289107009172439 accuracy: 83.8725\n",
      " batch 11000 lost: 1.6157010146379471 accuracy: 83.93181818181819\n",
      " batch 12000 lost: 1.6268023223876953 accuracy: 83.89166666666667\n",
      " batch 13000 lost: 1.6170311017036438 accuracy: 83.9326923076923\n",
      " batch 14000 lost: 1.6215904722213745 accuracy: 83.94464285714285\n",
      " batch 15000 lost: 1.619167030930519 accuracy: 83.96166666666667\n",
      "LOSS train 1.619167030930519, valid 1.6393208503723145\n",
      "ACCURACY train 83.96166666666667, valid 82.15\n",
      "EPOCH 16:\n",
      " batch 1000 lost: 1.6262164236307144 accuracy: 83.475\n",
      " batch 2000 lost: 1.6126829451322555 accuracy: 84.1625\n",
      " batch 3000 lost: 1.6157239674329757 accuracy: 84.3\n",
      " batch 4000 lost: 1.625788974046707 accuracy: 84.10625\n",
      " batch 5000 lost: 1.6192569680213929 accuracy: 84.135\n",
      " batch 6000 lost: 1.611938130259514 accuracy: 84.26666666666667\n",
      " batch 7000 lost: 1.625238122344017 accuracy: 84.17142857142858\n",
      " batch 8000 lost: 1.619035448908806 accuracy: 84.1875\n",
      " batch 9000 lost: 1.6205725393295287 accuracy: 84.16944444444445\n",
      " batch 10000 lost: 1.6248693857192993 accuracy: 84.1175\n",
      " batch 11000 lost: 1.6310998170375823 accuracy: 84.00909090909092\n",
      " batch 12000 lost: 1.6236307973861694 accuracy: 83.99166666666666\n",
      " batch 13000 lost: 1.6207356214523316 accuracy: 83.99807692307692\n",
      " batch 14000 lost: 1.6197607276439667 accuracy: 84.01607142857142\n",
      " batch 15000 lost: 1.6148585782051086 accuracy: 84.06333333333333\n",
      "LOSS train 1.6148585782051086, valid 1.6386816501617432\n",
      "ACCURACY train 84.06333333333333, valid 82.21\n",
      "EPOCH 17:\n",
      " batch 1000 lost: 1.614366532921791 accuracy: 84.75\n",
      " batch 2000 lost: 1.624889502286911 accuracy: 84.225\n",
      " batch 3000 lost: 1.6181212465763093 accuracy: 84.29166666666667\n",
      " batch 4000 lost: 1.6233487210273743 accuracy: 84.15\n",
      " batch 5000 lost: 1.6122814056873322 accuracy: 84.325\n",
      " batch 6000 lost: 1.6161000353097916 accuracy: 84.36666666666666\n",
      " batch 7000 lost: 1.61978186583519 accuracy: 84.34285714285714\n",
      " batch 8000 lost: 1.626469950914383 accuracy: 84.24375\n",
      " batch 9000 lost: 1.6223488686084748 accuracy: 84.21111111111111\n",
      " batch 10000 lost: 1.6153753979206085 accuracy: 84.2625\n",
      " batch 11000 lost: 1.6164153034687043 accuracy: 84.27954545454546\n",
      " batch 12000 lost: 1.6197933449745179 accuracy: 84.27708333333334\n",
      " batch 13000 lost: 1.619901573061943 accuracy: 84.26538461538462\n",
      " batch 14000 lost: 1.6171236633062362 accuracy: 84.27857142857142\n",
      " batch 15000 lost: 1.61679942381382 accuracy: 84.29\n",
      "LOSS train 1.61679942381382, valid 1.6391527652740479\n",
      "ACCURACY train 84.29, valid 82.14\n",
      "EPOCH 18:\n",
      " batch 1000 lost: 1.6157671661376953 accuracy: 84.675\n",
      " batch 2000 lost: 1.6280719269514083 accuracy: 84.05\n",
      " batch 3000 lost: 1.6171102718114854 accuracy: 84.2\n",
      " batch 4000 lost: 1.6094991326332093 accuracy: 84.49375\n",
      " batch 5000 lost: 1.6228630090951919 accuracy: 84.345\n",
      " batch 6000 lost: 1.6155243172645568 accuracy: 84.38333333333334\n",
      " batch 7000 lost: 1.6231408776044847 accuracy: 84.3\n",
      " batch 8000 lost: 1.6231645706892013 accuracy: 84.240625\n",
      " batch 9000 lost: 1.6119134237766266 accuracy: 84.33333333333333\n",
      " batch 10000 lost: 1.6075861352682113 accuracy: 84.4425\n",
      " batch 11000 lost: 1.627783229470253 accuracy: 84.35227272727273\n",
      " batch 12000 lost: 1.6172761560678481 accuracy: 84.36458333333333\n",
      " batch 13000 lost: 1.6174681743383408 accuracy: 84.36923076923077\n",
      " batch 14000 lost: 1.6176701248884202 accuracy: 84.36964285714286\n",
      " batch 15000 lost: 1.6200878146886826 accuracy: 84.36333333333333\n",
      "LOSS train 1.6200878146886826, valid 1.6339384317398071\n",
      "ACCURACY train 84.36333333333333, valid 82.69\n",
      "EPOCH 19:\n",
      " batch 1000 lost: 1.6244005181789398 accuracy: 83.65\n",
      " batch 2000 lost: 1.622360556125641 accuracy: 83.7875\n",
      " batch 3000 lost: 1.6066037938594817 accuracy: 84.36666666666666\n",
      " batch 4000 lost: 1.6131123299598693 accuracy: 84.49375\n",
      " batch 5000 lost: 1.6103404881954193 accuracy: 84.645\n",
      " batch 6000 lost: 1.6174043159484863 accuracy: 84.6\n",
      " batch 7000 lost: 1.621630878329277 accuracy: 84.51071428571429\n",
      " batch 8000 lost: 1.6114095993041992 accuracy: 84.575\n",
      " batch 9000 lost: 1.6212367243766785 accuracy: 84.53611111111111\n",
      " batch 10000 lost: 1.6251781225204467 accuracy: 84.445\n",
      " batch 11000 lost: 1.6153673205375672 accuracy: 84.45454545454545\n",
      " batch 12000 lost: 1.624819693684578 accuracy: 84.38541666666667\n",
      " batch 13000 lost: 1.6146400636434555 accuracy: 84.41153846153846\n",
      " batch 14000 lost: 1.6129975113868713 accuracy: 84.43571428571428\n",
      " batch 15000 lost: 1.6180010929107667 accuracy: 84.43\n",
      "LOSS train 1.6180010929107667, valid 1.6397056579589844\n",
      "ACCURACY train 84.43, valid 82.18\n",
      "EPOCH 20:\n",
      " batch 1000 lost: 1.6095683141946793 accuracy: 85.3\n",
      " batch 2000 lost: 1.606163687825203 accuracy: 85.4375\n",
      " batch 3000 lost: 1.6219897792339324 accuracy: 84.91666666666667\n",
      " batch 4000 lost: 1.6199946565628052 accuracy: 84.70625\n",
      " batch 5000 lost: 1.609889310002327 accuracy: 84.81\n",
      " batch 6000 lost: 1.623606004357338 accuracy: 84.64166666666667\n",
      " batch 7000 lost: 1.6242437628507613 accuracy: 84.53214285714286\n",
      " batch 8000 lost: 1.6237391694784165 accuracy: 84.44375\n",
      " batch 9000 lost: 1.6141164193153381 accuracy: 84.46944444444445\n",
      " batch 10000 lost: 1.6162373629808426 accuracy: 84.4575\n",
      " batch 11000 lost: 1.6206633781194686 accuracy: 84.42045454545455\n",
      " batch 12000 lost: 1.6207469098567964 accuracy: 84.38541666666667\n",
      " batch 13000 lost: 1.6136836774349212 accuracy: 84.42115384615384\n",
      " batch 14000 lost: 1.6247536766529083 accuracy: 84.36785714285715\n",
      " batch 15000 lost: 1.6050926723480226 accuracy: 84.46166666666667\n",
      "LOSS train 1.6050926723480226, valid 1.6354873180389404\n",
      "ACCURACY train 84.46166666666667, valid 82.51\n",
      "EPOCH 21:\n",
      " batch 1000 lost: 1.607344888806343 accuracy: 85.4\n",
      " batch 2000 lost: 1.6163314723968505 accuracy: 84.9125\n",
      " batch 3000 lost: 1.6173571310043335 accuracy: 84.75833333333334\n",
      " batch 4000 lost: 1.6133953052759171 accuracy: 84.75625\n",
      " batch 5000 lost: 1.6206317024230956 accuracy: 84.6\n",
      " batch 6000 lost: 1.6109877573251725 accuracy: 84.6875\n",
      " batch 7000 lost: 1.6230647609233857 accuracy: 84.56785714285714\n",
      " batch 8000 lost: 1.6202086453437805 accuracy: 84.5125\n",
      " batch 9000 lost: 1.6235943394899368 accuracy: 84.43888888888888\n",
      " batch 10000 lost: 1.6169799978733064 accuracy: 84.435\n",
      " batch 11000 lost: 1.6133124315738678 accuracy: 84.46136363636364\n",
      " batch 12000 lost: 1.6078165675401688 accuracy: 84.55208333333333\n",
      " batch 13000 lost: 1.6148475898504258 accuracy: 84.55961538461538\n",
      " batch 14000 lost: 1.6132602379322052 accuracy: 84.58392857142857\n",
      " batch 15000 lost: 1.615880108833313 accuracy: 84.585\n",
      "LOSS train 1.615880108833313, valid 1.6331675052642822\n",
      "ACCURACY train 84.585, valid 82.86\n",
      "EPOCH 22:\n",
      " batch 1000 lost: 1.6188535894155502 accuracy: 84.275\n",
      " batch 2000 lost: 1.6138446489572524 accuracy: 84.55\n",
      " batch 3000 lost: 1.6128256747722625 accuracy: 84.7\n",
      " batch 4000 lost: 1.6076621199846268 accuracy: 84.85625\n",
      " batch 5000 lost: 1.6183287842273713 accuracy: 84.725\n",
      " batch 6000 lost: 1.6067903299331665 accuracy: 84.84166666666667\n",
      " batch 7000 lost: 1.609239635825157 accuracy: 84.91071428571429\n",
      " batch 8000 lost: 1.6152129172086716 accuracy: 84.88125\n",
      " batch 9000 lost: 1.6219259914159774 accuracy: 84.77777777777777\n",
      " batch 10000 lost: 1.6146796227693558 accuracy: 84.775\n",
      " batch 11000 lost: 1.62027556681633 accuracy: 84.72045454545454\n",
      " batch 12000 lost: 1.6088015532493591 accuracy: 84.76666666666667\n",
      " batch 13000 lost: 1.6172126418352126 accuracy: 84.74807692307692\n",
      " batch 14000 lost: 1.6112357867956162 accuracy: 84.76785714285714\n",
      " batch 15000 lost: 1.618195386171341 accuracy: 84.74166666666666\n",
      "LOSS train 1.618195386171341, valid 1.6394187211990356\n",
      "ACCURACY train 84.74166666666666, valid 82.22\n",
      "EPOCH 23:\n",
      " batch 1000 lost: 1.6100002857446671 accuracy: 85.05\n",
      " batch 2000 lost: 1.6131798802614212 accuracy: 84.9875\n",
      " batch 3000 lost: 1.6296317818164825 accuracy: 84.375\n",
      " batch 4000 lost: 1.6094883462190628 accuracy: 84.6\n",
      " batch 5000 lost: 1.6197411350011826 accuracy: 84.5\n",
      " batch 6000 lost: 1.6203308461904526 accuracy: 84.43333333333334\n",
      " batch 7000 lost: 1.612691942691803 accuracy: 84.49285714285715\n",
      " batch 8000 lost: 1.6020967170000076 accuracy: 84.66875\n",
      " batch 9000 lost: 1.6142431007623672 accuracy: 84.68611111111112\n",
      " batch 10000 lost: 1.6028864097595215 accuracy: 84.81\n",
      " batch 11000 lost: 1.6174196150302886 accuracy: 84.77727272727273\n",
      " batch 12000 lost: 1.617009631037712 accuracy: 84.76875\n",
      " batch 13000 lost: 1.6002733078002929 accuracy: 84.86346153846154\n",
      " batch 14000 lost: 1.6172983170747757 accuracy: 84.83214285714286\n",
      " batch 15000 lost: 1.6261291072368622 accuracy: 84.75166666666667\n",
      "LOSS train 1.6261291072368622, valid 1.6381596326828003\n",
      "ACCURACY train 84.75166666666667, valid 82.3\n",
      "EPOCH 24:\n",
      " batch 1000 lost: 1.624842495560646 accuracy: 83.725\n",
      " batch 2000 lost: 1.6059490286111833 accuracy: 84.5875\n",
      " batch 3000 lost: 1.610580899119377 accuracy: 84.75833333333334\n",
      " batch 4000 lost: 1.6058255414962768 accuracy: 84.9875\n",
      " batch 5000 lost: 1.6191324889659882 accuracy: 84.825\n",
      " batch 6000 lost: 1.615050386071205 accuracy: 84.8\n",
      " batch 7000 lost: 1.6027096482515335 accuracy: 84.94285714285714\n",
      " batch 8000 lost: 1.6153245842456818 accuracy: 84.9\n",
      " batch 9000 lost: 1.6108273384571075 accuracy: 84.90833333333333\n",
      " batch 10000 lost: 1.6145138932466507 accuracy: 84.895\n",
      " batch 11000 lost: 1.6157437701225281 accuracy: 84.86363636363636\n",
      " batch 12000 lost: 1.6154724522829056 accuracy: 84.84791666666666\n",
      " batch 13000 lost: 1.6113807601928711 accuracy: 84.85961538461538\n",
      " batch 14000 lost: 1.6113828638792038 accuracy: 84.86964285714286\n",
      " batch 15000 lost: 1.6146984974145888 accuracy: 84.85\n",
      "LOSS train 1.6146984974145888, valid 1.6311100721359253\n",
      "ACCURACY train 84.85, valid 82.97\n",
      "EPOCH 25:\n",
      " batch 1000 lost: 1.6122585639953613 accuracy: 84.975\n",
      " batch 2000 lost: 1.608278671503067 accuracy: 85.1875\n",
      " batch 3000 lost: 1.6150610978603364 accuracy: 84.96666666666667\n",
      " batch 4000 lost: 1.6182661607265472 accuracy: 84.825\n",
      " batch 5000 lost: 1.608675850868225 accuracy: 84.92\n",
      " batch 6000 lost: 1.6086610044240952 accuracy: 84.97916666666667\n",
      " batch 7000 lost: 1.597520871758461 accuracy: 85.18928571428572\n",
      " batch 8000 lost: 1.6121507456302644 accuracy: 85.15625\n",
      " batch 9000 lost: 1.6147580225467681 accuracy: 85.10277777777777\n",
      " batch 10000 lost: 1.6108959571123123 accuracy: 85.11\n",
      " batch 11000 lost: 1.621333847641945 accuracy: 85.00227272727273\n",
      " batch 12000 lost: 1.6140604459047319 accuracy: 84.98125\n",
      " batch 13000 lost: 1.6105601388216018 accuracy: 84.99807692307692\n",
      " batch 14000 lost: 1.6130201449394226 accuracy: 84.9875\n",
      " batch 15000 lost: 1.617042477965355 accuracy: 84.95333333333333\n",
      "LOSS train 1.617042477965355, valid 1.6358466148376465\n",
      "ACCURACY train 84.95333333333333, valid 82.5\n",
      "EPOCH 26:\n",
      " batch 1000 lost: 1.6144456655979156 accuracy: 84.675\n",
      " batch 2000 lost: 1.6143421992063522 accuracy: 84.6875\n",
      " batch 3000 lost: 1.6081990077495576 accuracy: 84.90833333333333\n",
      " batch 4000 lost: 1.604892679810524 accuracy: 85.1125\n",
      " batch 5000 lost: 1.6113140839338302 accuracy: 85.1\n",
      " batch 6000 lost: 1.6158992151021958 accuracy: 85.00833333333334\n",
      " batch 7000 lost: 1.6052320020198823 accuracy: 85.10357142857143\n",
      " batch 8000 lost: 1.6198287525177002 accuracy: 84.98125\n",
      " batch 9000 lost: 1.6138297052383423 accuracy: 84.95833333333333\n",
      " batch 10000 lost: 1.6101690672636033 accuracy: 84.965\n",
      " batch 11000 lost: 1.6056922628879546 accuracy: 85.02954545454546\n",
      " batch 12000 lost: 1.6157205867767335 accuracy: 84.98958333333333\n",
      " batch 13000 lost: 1.6108095886707305 accuracy: 85.00192307692308\n",
      " batch 14000 lost: 1.6136433538198471 accuracy: 84.98571428571428\n",
      " batch 15000 lost: 1.610022954106331 accuracy: 84.99333333333334\n",
      "LOSS train 1.610022954106331, valid 1.632645845413208\n",
      "ACCURACY train 84.99333333333334, valid 82.77\n",
      "EPOCH 27:\n",
      " batch 1000 lost: 1.618437665462494 accuracy: 84.3\n",
      " batch 2000 lost: 1.6092446154356004 accuracy: 84.7125\n",
      " batch 3000 lost: 1.6159459223747255 accuracy: 84.66666666666667\n",
      " batch 4000 lost: 1.619731941819191 accuracy: 84.53125\n",
      " batch 5000 lost: 1.6146741095781327 accuracy: 84.55\n",
      " batch 6000 lost: 1.6148062381744386 accuracy: 84.55\n",
      " batch 7000 lost: 1.6057906703948974 accuracy: 84.69285714285714\n",
      " batch 8000 lost: 1.606436763048172 accuracy: 84.78125\n",
      " batch 9000 lost: 1.6055232156515122 accuracy: 84.86666666666666\n",
      " batch 10000 lost: 1.6125595253705978 accuracy: 84.875\n",
      " batch 11000 lost: 1.6106318933963775 accuracy: 84.89545454545454\n",
      " batch 12000 lost: 1.601071941614151 accuracy: 84.98958333333333\n",
      " batch 13000 lost: 1.6104949066638947 accuracy: 84.99615384615385\n",
      " batch 14000 lost: 1.604726309657097 accuracy: 85.03928571428571\n",
      " batch 15000 lost: 1.6149335309267043 accuracy: 85.01166666666667\n",
      "LOSS train 1.6149335309267043, valid 1.6319390535354614\n",
      "ACCURACY train 85.01166666666667, valid 82.89\n",
      "EPOCH 28:\n",
      " batch 1000 lost: 1.6038837506771089 accuracy: 85.775\n",
      " batch 2000 lost: 1.6133814886808395 accuracy: 85.2875\n",
      " batch 3000 lost: 1.6015475388765334 accuracy: 85.54166666666667\n",
      " batch 4000 lost: 1.6082670755386352 accuracy: 85.48125\n",
      " batch 5000 lost: 1.6075683043003082 accuracy: 85.475\n",
      " batch 6000 lost: 1.6077192641496658 accuracy: 85.46666666666667\n",
      " batch 7000 lost: 1.6072014763355256 accuracy: 85.46071428571429\n",
      " batch 8000 lost: 1.6198888399600984 accuracy: 85.303125\n",
      " batch 9000 lost: 1.6163989561796188 accuracy: 85.20277777777778\n",
      " batch 10000 lost: 1.6159265279769897 accuracy: 85.15\n",
      " batch 11000 lost: 1.6097144738435745 accuracy: 85.15681818181818\n",
      " batch 12000 lost: 1.6089037598371505 accuracy: 85.16875\n",
      " batch 13000 lost: 1.606884782075882 accuracy: 85.1923076923077\n",
      " batch 14000 lost: 1.6110638152360917 accuracy: 85.175\n",
      " batch 15000 lost: 1.607720876097679 accuracy: 85.18833333333333\n",
      "LOSS train 1.607720876097679, valid 1.6301769018173218\n",
      "ACCURACY train 85.18833333333333, valid 83.08\n",
      "EPOCH 29:\n",
      " batch 1000 lost: 1.603018922805786 accuracy: 85.85\n",
      " batch 2000 lost: 1.6056810978651046 accuracy: 85.75\n",
      " batch 3000 lost: 1.602918415427208 accuracy: 85.78333333333333\n",
      " batch 4000 lost: 1.6145594335794449 accuracy: 85.51875\n",
      " batch 5000 lost: 1.6116819700002671 accuracy: 85.435\n",
      " batch 6000 lost: 1.601133936405182 accuracy: 85.55833333333334\n",
      " batch 7000 lost: 1.6001205755472183 accuracy: 85.66071428571429\n",
      " batch 8000 lost: 1.611748496413231 accuracy: 85.565625\n",
      " batch 9000 lost: 1.6166152764558792 accuracy: 85.44722222222222\n",
      " batch 10000 lost: 1.613864640712738 accuracy: 85.38\n",
      " batch 11000 lost: 1.618525755763054 accuracy: 85.2909090909091\n",
      " batch 12000 lost: 1.6047402777671813 accuracy: 85.32083333333334\n",
      " batch 13000 lost: 1.6129169142246247 accuracy: 85.28076923076924\n",
      " batch 14000 lost: 1.6103081183433532 accuracy: 85.27321428571429\n",
      " batch 15000 lost: 1.618262531518936 accuracy: 85.21166666666667\n",
      "LOSS train 1.618262531518936, valid 1.630496859550476\n",
      "ACCURACY train 85.21166666666667, valid 82.98\n",
      "EPOCH 30:\n",
      " batch 1000 lost: 1.6150592983961105 accuracy: 84.8\n",
      " batch 2000 lost: 1.6164434803724288 accuracy: 84.6\n",
      " batch 3000 lost: 1.6082662249803543 accuracy: 84.81666666666666\n",
      " batch 4000 lost: 1.611121074438095 accuracy: 84.875\n",
      " batch 5000 lost: 1.6078038451671601 accuracy: 84.985\n",
      " batch 6000 lost: 1.6006319807767868 accuracy: 85.17083333333333\n",
      " batch 7000 lost: 1.5959163275957107 accuracy: 85.36428571428571\n",
      " batch 8000 lost: 1.613350496172905 accuracy: 85.2875\n",
      " batch 9000 lost: 1.6086570262908935 accuracy: 85.28333333333333\n",
      " batch 10000 lost: 1.6125076175928117 accuracy: 85.2425\n",
      " batch 11000 lost: 1.6091007294654847 accuracy: 85.25227272727273\n",
      " batch 12000 lost: 1.6151577157974244 accuracy: 85.2\n",
      " batch 13000 lost: 1.6038926196098329 accuracy: 85.24230769230769\n",
      " batch 14000 lost: 1.601593983054161 accuracy: 85.29821428571428\n",
      " batch 15000 lost: 1.6156244856119155 accuracy: 85.25\n",
      "LOSS train 1.6156244856119155, valid 1.6280699968338013\n",
      "ACCURACY train 85.25, valid 83.35\n"
     ]
    }
   ],
   "source": [
    "# Actual training loop: Equivalent to tensorflow.fit\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "best_vloss = 1_000_000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch+1}:')\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, accuracy = one_epoch_training(epoch, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    # Set the model to evaluation mode, disabling dropout and using population statistics for batch normalization\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption\n",
    "    # Calculate validation loss here\n",
    "    with torch.no_grad():\n",
    "        vcorrect = 0\n",
    "        vsamples = 0\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            # vinputs = vinputs.to(device)\n",
    "            # vlabels = vlabels.to(device)\n",
    "            vpredictions = model(vinputs)\n",
    "            _, vpredict = torch.max(vpredictions, 1)\n",
    "            vloss = loss_fn(vpredictions, vlabels)\n",
    "            running_vloss += vloss\n",
    "            vcorrect += (vpredict == vlabels).sum().item()\n",
    "            vsamples += vlabels.size(0)\n",
    "\n",
    "    vaccuracy = 100 * vcorrect / vsamples\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train {avg_loss}, valid {avg_vloss}')\n",
    "    print(f'ACCURACY train {accuracy}, valid {vaccuracy}')\n",
    "\n",
    "    writer.add_scalars('Training vs. Validation Loss', {\n",
    "        'Training' : avg_loss, 'Validation' : avg_vloss\n",
    "    }, epoch + 1)\n",
    "    writer.add_scalars('Training vs. Validation Accuracy', {\n",
    "        'Training' : accuracy, 'Validation' : vaccuracy\n",
    "    }, epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the best model state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f'Models/FashionMNIST/fashion_model_{timestamp}_{epoch}'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043c097-0e46-440e-bee2-0fc278162e15",
   "metadata": {},
   "source": [
    "use tensorboard --lodir=\"runs/\" to view accuracy and loss graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce17ac-7676-4d27-8127-ef0c9708ee6b",
   "metadata": {},
   "source": [
    "Overall the accuracy is similar to the TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "62d37536-1ce3-47e5-98da-5d313b944551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11610, 8])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch california house data\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using previous housing to be consistent with thje book\n",
    "housing = fetch_california_housing()\n",
    "cal_x_train_full, cal_x_test, cal_y_train_full, cal_y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "cal_x_train, cal_x_val, cal_y_train, cal_y_val = train_test_split(\n",
    "    cal_x_train_full, cal_y_train_full, random_state=42)\n",
    "\n",
    "cal_x_train = torch.Tensor(cal_x_train)\n",
    "cal_y_train = torch.Tensor(cal_y_train)\n",
    "cal_x_test = torch.Tensor(cal_x_test)\n",
    "cal_y_test = torch.Tensor(cal_y_test)\n",
    "cal_x_val = torch.Tensor(cal_x_val)\n",
    "cal_y_val = torch.Tensor(cal_y_val)\n",
    "cal_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e6e03968-34f9-42ee-a354-782bed44e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make CustomDataSet\n",
    "\n",
    "class CalHousingDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, values):\n",
    "        self.features = features\n",
    "        self.values = values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        feature = (feature - feature.mean()) / feature.std()\n",
    "        value = self.values[index]\n",
    "        return feature, value\n",
    "        \n",
    "cal_training = CalHousingDataSet(cal_x_train, cal_y_train)\n",
    "cal_validation = CalHousingDataSet(cal_x_val, cal_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5e64c200-1642-4401-a14f-14b029160b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Data Loaders\n",
    "batch_size = 32\n",
    "cal_train_dataloader = torch.utils.data.DataLoader(cal_training, batch_size=batch_size, shuffle=True)\n",
    "cal_val_dataloader = torch.utils.data.DataLoader(cal_validation, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8a9f9c85-97ab-4470-89f6-f7c1b34304d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalHousingPredictor(\n",
      "  (sequential): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "class CalHousingPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CalHousingPredictor, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(8, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.sequential(x)\n",
    "        return logits\n",
    "\n",
    "cal_housing_model = CalHousingPredictor()\n",
    "print(cal_housing_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "58d4d00b-28b6-47a2-bdea-8c4282e962cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Loss\n",
    "cal_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Choose Optimizer\n",
    "cal_optim_fn = torch.optim.Adam(cal_housing_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3584bd95-b55a-467b-b4d0-6db004c08039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Epoch Functionality\n",
    "# One epoch training function\n",
    "def one_epoch_training_regress(epoch_index, tb_writer):\n",
    "    '''\n",
    "        Parameters:\n",
    "            epoch_index: Specifies the current epoch being trained\n",
    "            tb_writer: Specifies the TensorBoard Writer for visualization\n",
    "\n",
    "        Returns:\n",
    "            Final loss value for epoch\n",
    "    '''\n",
    "    running_loss = 0.\n",
    "    final_loss = 0.\n",
    "\n",
    "    # Loop through all batches of images \n",
    "    for i, data in enumerate(cal_train_dataloader):\n",
    "        inputs, labels = data\n",
    "        # inputs = inputs.to(device)\n",
    "        # labels = labels.to(device)\n",
    "\n",
    "        cal_optim_fn.zero_grad()\n",
    "\n",
    "        predictions = cal_housing_model(inputs)\n",
    "\n",
    "        loss = cal_loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        cal_optim_fn.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            final_loss = running_loss / 1000 # loss per batch\n",
    "            print(f' batch {i+1} lost: {final_loss}')\n",
    "            samples_trained = epoch_index * len(cal_train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', final_loss, samples_trained)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "66d1f210-96a6-4f31-b06b-b3b2af24d0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blake/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 1000 lost: 1.4577735597752035\n",
      " batch 2000 lost: 1.3249137233104558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blake/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 1.3249137233104558, valid 1.3602584600448608\n",
      "EPOCH 2:\n",
      " batch 1000 lost: 1.3298056891150773\n",
      " batch 2000 lost: 1.3752771702092141\n",
      "LOSS train 1.3752771702092141, valid 1.3598235845565796\n",
      "EPOCH 3:\n",
      " batch 1000 lost: 1.379435848876834\n",
      " batch 2000 lost: 1.3762845395542682\n",
      "LOSS train 1.3762845395542682, valid 1.3158090114593506\n",
      "EPOCH 4:\n",
      " batch 1000 lost: 1.381956454642117\n",
      " batch 2000 lost: 1.3762726607881486\n",
      "LOSS train 1.3762726607881486, valid 1.3166735172271729\n",
      "EPOCH 5:\n",
      " batch 1000 lost: 1.4061785711795092\n",
      " batch 2000 lost: 1.3817301841564475\n",
      "LOSS train 1.3817301841564475, valid 1.3332821130752563\n",
      "EPOCH 6:\n",
      " batch 1000 lost: 1.3345778232701122\n",
      " batch 2000 lost: 1.392570194265805\n",
      "LOSS train 1.392570194265805, valid 1.324141025543213\n",
      "EPOCH 7:\n",
      " batch 1000 lost: 1.3348405571430921\n",
      " batch 2000 lost: 1.3682498428281398\n",
      "LOSS train 1.3682498428281398, valid 1.407766580581665\n",
      "EPOCH 8:\n",
      " batch 1000 lost: 1.39646395047009\n",
      " batch 2000 lost: 1.4019661042722873\n",
      "LOSS train 1.4019661042722873, valid 1.352418303489685\n",
      "EPOCH 9:\n",
      " batch 1000 lost: 1.3926275747045875\n",
      " batch 2000 lost: 1.3558918651323766\n",
      "LOSS train 1.3558918651323766, valid 1.3663357496261597\n",
      "EPOCH 10:\n",
      " batch 1000 lost: 1.3382379942759872\n",
      " batch 2000 lost: 1.389719738461077\n",
      "LOSS train 1.389719738461077, valid 1.338334321975708\n",
      "EPOCH 11:\n",
      " batch 1000 lost: 1.3571955198571086\n",
      " batch 2000 lost: 1.3960483858799562\n",
      "LOSS train 1.3960483858799562, valid 1.333589792251587\n",
      "EPOCH 12:\n",
      " batch 1000 lost: 1.3633778588669374\n",
      " batch 2000 lost: 1.3383389866389335\n",
      "LOSS train 1.3383389866389335, valid 1.3298109769821167\n",
      "EPOCH 13:\n",
      " batch 1000 lost: 1.3814146289378404\n",
      " batch 2000 lost: 1.3500991392182187\n",
      "LOSS train 1.3500991392182187, valid 1.3276188373565674\n",
      "EPOCH 14:\n",
      " batch 1000 lost: 1.3681022003777326\n",
      " batch 2000 lost: 1.395014827873558\n",
      "LOSS train 1.395014827873558, valid 1.327125072479248\n",
      "EPOCH 15:\n",
      " batch 1000 lost: 1.3489780879197641\n",
      " batch 2000 lost: 1.424007251502946\n",
      "LOSS train 1.424007251502946, valid 1.3146824836730957\n",
      "EPOCH 16:\n",
      " batch 1000 lost: 1.3351776705975644\n",
      " batch 2000 lost: 1.3824108059238642\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m cal_housing_model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mone_epoch_training_regress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population statistics for batch normalization\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[193], line 28\u001b[0m, in \u001b[0;36mone_epoch_training_regress\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m cal_loss_fn(predictions, labels)\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m \u001b[43mcal_optim_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m999\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:385\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    384\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 385\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    388\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Actual training loop: Equivalent to tensorflow.fit\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/cal_housing_{}'.format(timestamp))\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "best_vloss = 1_000_000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch+1}:')\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    cal_housing_model.train(True)\n",
    "    avg_loss = one_epoch_training_regress(epoch, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    # Set the model to evaluation mode, disabling dropout and using population statistics for batch normalization\n",
    "    cal_housing_model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption\n",
    "    # Calculate validation loss here\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(cal_val_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            # vinputs = vinputs.to(device)\n",
    "            # vlabels = vlabels.to(device)\n",
    "            vpredictions = cal_housing_model(vinputs)\n",
    "            vloss = cal_loss_fn(vpredictions, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train {avg_loss}, valid {avg_vloss}')\n",
    "\n",
    "    writer.add_scalars('Training vs. Validation Loss', {\n",
    "        'Training' : avg_loss, 'Validation' : avg_vloss\n",
    "    }, epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the best model state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f'Models/CalHousing/cal_housing_model_{timestamp}_{epoch}'\n",
    "        torch.save(cal_housing_model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90f010-807d-4eb3-9a22-d67844a8d1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484cecf6-5937-47fe-a3de-2a4cab922780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
