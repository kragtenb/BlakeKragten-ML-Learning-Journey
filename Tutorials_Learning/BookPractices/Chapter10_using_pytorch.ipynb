{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d31138-1fdf-4553-a3d3-a1f8d4af627c",
   "metadata": {},
   "source": [
    "# This document will implement the Chapter 10 models and exercises but will use pytorch instead of tensorflow and keras\n",
    "## Fashion NIST Simple NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b16cd86-1a9e-46ff-892b-e758799b1e94",
   "metadata": {},
   "source": [
    "In pytorch, instead of creating a model like in tensor flow that is stand alone from APIs. It looks like it is more object oriented and uses a class structure instead. \n",
    "\n",
    "First step will be to get the fashion data into a format that can be used with the model itself. This requires the creation of Dataset & Dataloader.\n",
    "\n",
    "Dataset is responsible for accessing and processing single instances of the data. \n",
    "\n",
    "While a Dataloader pulls instaces of the data from the Dataset, collects them in batches and returns them for use in training.\n",
    "\n",
    "Part of the dataset creation, we will normalize the images to be zero centered as these are more efficient to work with causing training loop to be shorter in runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33711b3f-9087-43ce-a8ca-464a03242ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Dataset and DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd1075e4-476e-4506-b848-83677c73a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 09:39:24.662753: E external/local_xla/xla/stream_executor/plugin_registry.cc:93] Invalid plugin kind specified: DNN\n",
      "2024-05-10 09:39:24.701228: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Add tensor board support for better visualization of the model we are creating.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0226d5-f712-49d8-b580-6aeb03aed7ae",
   "metadata": {},
   "source": [
    "With pytorch we can use GPU for faster training speed. Figure out if we can with this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94823ca-e350-4d3e-85ff-97ac16fb3a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028db700-4bec-473e-bc93-0a4a625da18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "investigate_mnist_data_shape = torchvision.datasets.FashionMNIST('Data/FashionMNIST', train=True, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfaaf874-2541-4177-a22b-c8fcee712d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate the size of the data\n",
    "investigate_mnist_data_shape.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3e8ce-f8b5-42eb-ad58-f7330ca5369a",
   "metadata": {},
   "source": [
    "The data is 60_000 images of the shape (28, 28). Thus for the normalized transform we need to have to dimensions for the (mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3557774f-4978-446c-89b7-a53ebf749cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a transformer function. This will convert the dataset to a tensor and\n",
    "# apply the normalization to make it 0 centered and standard deviation around 0.5\n",
    "\n",
    "mean = 0.5\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize needs a mean for each channel. Being that this is a grayscale we only have one channel\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ]    \n",
    ")\n",
    "\n",
    "training_set = torchvision.datasets.FashionMNIST('Data/FashionMNIST', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('Data/FashionMNIST', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b476b1-bc0a-41fb-8f73-65ba378821f8",
   "metadata": {},
   "source": [
    "We now have our datasets. As the data was already in torchvision we don't need to explicitly create the classes themselves yet.\n",
    "\n",
    "Now we need to create a DataLoader that will take in this DataSet for both training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7cc7820-a534-4308-b286-504253869f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67826a7a-b680-4ea4-b16c-daf5f7cd349a",
   "metadata": {},
   "source": [
    "Generate our class labels as the FashionMNIST data set does not have any labels, just numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "361ee6d2-6838-4246-8d81-b0b87ff23c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbac035-df62-4c13-aca4-df64cbb329cd",
   "metadata": {},
   "source": [
    "To make sure the data is looking like how we should expect we should visualize the data we received using MatPlotLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a8976d-dc75-45f0-a95a-7f7ee6bfb9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trouser Shirt T-shirt/top Ankle Boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_image(img):\n",
    "    img = img.mean(dim=0)\n",
    "    # Remove normalization\n",
    "    img = img / 2 + 0.5\n",
    "    # Convert from tensor to np.array\n",
    "    img_np = img.numpy()\n",
    "    plt.imshow(img_np, cmap='gray')\n",
    "\n",
    "# Data iterator of the data loader\n",
    "data_iter = iter(training_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Create a grid of the images to be displayed\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "display_image(img_grid)\n",
    "\n",
    "print(' '.join(classes[label] for label in labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be349db1-dc48-4898-8c58-69bba438baff",
   "metadata": {},
   "source": [
    "The labels and the data matches. Sanity Check complete. Good to move forware to setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1c0260c-459d-425b-b1a1-81ecb8333350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neural network and activation function libraries\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba02f5c-8b0e-4579-9c7e-951fa0811be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch models are created by inheriting nn.Module\n",
    "class FashionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(28*28, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.sequential(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a08200bc-8a32-4bf8-89d7-7b87c010bd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionClassifier(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (sequential): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "    (5): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FashionClassifier()\n",
    "model = FashionClassifier().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4867f1-0563-4390-a10b-48cd74759099",
   "metadata": {},
   "source": [
    "Woot, model looks fairly similar to the model we created in the TensorFlow Document\n",
    "\n",
    "Now it is time to train the model. In pytorch the steps are more drawn out. Instead of doing fit like in tensorflow we need to list each step in the training loop and make the loop as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb29eafd-8843-45ca-91c4-9092ee5bb8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Get Optimizer, Using Parameters from tensorflow example\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6fdca-0e7f-41f9-abfe-ed5908a92f14",
   "metadata": {},
   "source": [
    "For the training loop the general flow is as follows:\n",
    "1. Get a batch of data from Dataloader\n",
    "2. Zero optimizer's gradient\n",
    "3. Query model for predictions\n",
    "4. Calculate loss between predictions and labels\n",
    "5. Calculate backward gradients over the learning weights\n",
    "6. Tell the optimizer to perform 1 step to adjust the weights based on the backwards gradients\n",
    "7. Report on loss and average loss every to often to see it is working (1_000 batches) and not too often to cause too much non-gpu time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e36fa153-358d-4e84-ac60-7870953fcaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One epoch training function\n",
    "def one_epoch_training(epoch_index, tb_writer):\n",
    "    '''\n",
    "        Parameters:\n",
    "            epoch_index: Specifies the current epoch being trained\n",
    "            tb_writer: Specifies the TensorBoard Writer for visualization\n",
    "\n",
    "        Returns:\n",
    "            Final loss value for epoch\n",
    "    '''\n",
    "    running_loss = 0.\n",
    "    final_loss = 0.\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Loop through all batches of images \n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(inputs)\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Update the running total of correct predictions and samples\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        if i % 1000 == 999:\n",
    "            final_loss = running_loss / 1000 # loss per batch\n",
    "            accuracy = 100 * total_correct / total_samples\n",
    "            print(f' batch {i+1} lost: {final_loss} accuracy: {accuracy}')\n",
    "            images_trained = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train/accuracy', final_loss, images_trained, accuracy)\n",
    "            running_loss = 0.\n",
    "\n",
    "    accuracy = 100 * total_correct / total_samples\n",
    "    return final_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1eeae5d-c787-49bb-be32-4704674c1bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      " batch 1000 lost: 2.2449551124572755 accuracy: 27.675\n",
      " batch 2000 lost: 1.9876223553419112 accuracy: 39.825\n",
      " batch 3000 lost: 1.7985867702960967 accuracy: 50.05\n",
      " batch 4000 lost: 1.7526023719310762 accuracy: 55.85\n",
      " batch 5000 lost: 1.7265358922481537 accuracy: 59.63\n",
      " batch 6000 lost: 1.7182517342567445 accuracy: 62.24166666666667\n",
      " batch 7000 lost: 1.706599822640419 accuracy: 64.23571428571428\n",
      " batch 8000 lost: 1.706416628241539 accuracy: 65.765625\n",
      " batch 9000 lost: 1.6912328671216965 accuracy: 67.09166666666667\n",
      " batch 10000 lost: 1.6833318072557448 accuracy: 68.2525\n",
      " batch 11000 lost: 1.6913210563659669 accuracy: 69.125\n",
      " batch 12000 lost: 1.6793804508447647 accuracy: 69.9375\n",
      " batch 13000 lost: 1.6864187961816788 accuracy: 70.5576923076923\n",
      " batch 14000 lost: 1.6819465032815932 accuracy: 71.1125\n",
      " batch 15000 lost: 1.6724706066846848 accuracy: 71.67166666666667\n",
      "LOSS train 1.6724706066846848, valid 1.680990219116211\n",
      "ACCURACY train 71.67166666666667, valid 78.29\n",
      "EPOCH 2:\n",
      " batch 1000 lost: 1.6682519114017487 accuracy: 79.825\n",
      " batch 2000 lost: 1.666080410838127 accuracy: 79.925\n",
      " batch 3000 lost: 1.6653472796678543 accuracy: 79.925\n",
      " batch 4000 lost: 1.6710566725730895 accuracy: 79.7875\n",
      " batch 5000 lost: 1.682319857954979 accuracy: 79.485\n",
      " batch 6000 lost: 1.674284857749939 accuracy: 79.39166666666667\n",
      " batch 7000 lost: 1.6692170802354813 accuracy: 79.39642857142857\n",
      " batch 8000 lost: 1.6647995866537093 accuracy: 79.459375\n",
      " batch 9000 lost: 1.6619701713323594 accuracy: 79.51944444444445\n",
      " batch 10000 lost: 1.657872926592827 accuracy: 79.6425\n",
      " batch 11000 lost: 1.6488252178430558 accuracy: 79.81363636363636\n",
      " batch 12000 lost: 1.6697338756322861 accuracy: 79.77083333333333\n",
      " batch 13000 lost: 1.6573113008737563 accuracy: 79.82884615384616\n",
      " batch 14000 lost: 1.6585940380096436 accuracy: 79.89107142857142\n",
      " batch 15000 lost: 1.6648471242189407 accuracy: 79.88333333333334\n",
      "LOSS train 1.6648471242189407, valid 1.6623939275741577\n",
      "ACCURACY train 79.88333333333334, valid 79.92\n",
      "EPOCH 3:\n",
      " batch 1000 lost: 1.6553612074851989 accuracy: 80.8\n",
      " batch 2000 lost: 1.6566109659671784 accuracy: 80.7875\n",
      " batch 3000 lost: 1.6552338954210282 accuracy: 80.825\n",
      " batch 4000 lost: 1.6594930920600892 accuracy: 80.68125\n",
      " batch 5000 lost: 1.664434708714485 accuracy: 80.495\n",
      " batch 6000 lost: 1.6466694036722183 accuracy: 80.62916666666666\n",
      " batch 7000 lost: 1.658220491886139 accuracy: 80.60714285714286\n",
      " batch 8000 lost: 1.651616621851921 accuracy: 80.6875\n",
      " batch 9000 lost: 1.6617740329504014 accuracy: 80.60555555555555\n",
      " batch 10000 lost: 1.6568504199981688 accuracy: 80.595\n",
      " batch 11000 lost: 1.6500953367948532 accuracy: 80.6590909090909\n",
      " batch 12000 lost: 1.6552779344320296 accuracy: 80.66875\n",
      " batch 13000 lost: 1.6510359010696412 accuracy: 80.72692307692307\n",
      " batch 14000 lost: 1.6556967220306396 accuracy: 80.72142857142858\n",
      " batch 15000 lost: 1.6545120441913606 accuracy: 80.725\n",
      "LOSS train 1.6545120441913606, valid 1.6599342823028564\n",
      "ACCURACY train 80.725, valid 80.28\n",
      "EPOCH 4:\n",
      " batch 1000 lost: 1.6492693964242935 accuracy: 81.25\n",
      " batch 2000 lost: 1.6435090212821961 accuracy: 81.65\n",
      " batch 3000 lost: 1.6461172997951508 accuracy: 81.65\n",
      " batch 4000 lost: 1.6501294424533843 accuracy: 81.56875\n",
      " batch 5000 lost: 1.6621173009872436 accuracy: 81.24\n",
      " batch 6000 lost: 1.6511812673807145 accuracy: 81.21666666666667\n",
      " batch 7000 lost: 1.6437858774662018 accuracy: 81.35714285714286\n",
      " batch 8000 lost: 1.6485801255702972 accuracy: 81.346875\n",
      " batch 9000 lost: 1.6401759119033814 accuracy: 81.44722222222222\n",
      " batch 10000 lost: 1.6459628740549088 accuracy: 81.4475\n",
      " batch 11000 lost: 1.6429949904680252 accuracy: 81.475\n",
      " batch 12000 lost: 1.641533149123192 accuracy: 81.52291666666666\n",
      " batch 13000 lost: 1.647355558037758 accuracy: 81.52307692307693\n",
      " batch 14000 lost: 1.6535925097465516 accuracy: 81.47142857142858\n",
      " batch 15000 lost: 1.6597691643238068 accuracy: 81.38666666666667\n",
      "LOSS train 1.6597691643238068, valid 1.6515042781829834\n",
      "ACCURACY train 81.38666666666667, valid 81.07\n",
      "EPOCH 5:\n",
      " batch 1000 lost: 1.638879731655121 accuracy: 82.45\n",
      " batch 2000 lost: 1.6358497060537338 accuracy: 82.5375\n",
      " batch 3000 lost: 1.6366539980173112 accuracy: 82.6\n",
      " batch 4000 lost: 1.6423830733299256 accuracy: 82.5\n",
      " batch 5000 lost: 1.6528619894981385 accuracy: 82.19\n",
      " batch 6000 lost: 1.6489521094560624 accuracy: 82.05833333333334\n",
      " batch 7000 lost: 1.652771265745163 accuracy: 81.91071428571429\n",
      " batch 8000 lost: 1.6492202447652817 accuracy: 81.846875\n",
      " batch 9000 lost: 1.651228616118431 accuracy: 81.78055555555555\n",
      " batch 10000 lost: 1.6301026819944382 accuracy: 81.9375\n",
      " batch 11000 lost: 1.6466640875339509 accuracy: 81.91363636363636\n",
      " batch 12000 lost: 1.6432599250078201 accuracy: 81.93333333333334\n",
      " batch 13000 lost: 1.6483159801959992 accuracy: 81.89038461538462\n",
      " batch 14000 lost: 1.6419497200250626 accuracy: 81.88928571428572\n",
      " batch 15000 lost: 1.632390855908394 accuracy: 81.96166666666667\n",
      "LOSS train 1.632390855908394, valid 1.6500110626220703\n",
      "ACCURACY train 81.96166666666667, valid 81.19\n",
      "EPOCH 6:\n",
      " batch 1000 lost: 1.6368508769273757 accuracy: 82.575\n",
      " batch 2000 lost: 1.6363488885164261 accuracy: 82.65\n",
      " batch 3000 lost: 1.6364906042814256 accuracy: 82.63333333333334\n",
      " batch 4000 lost: 1.6331653653383256 accuracy: 82.69375\n",
      " batch 5000 lost: 1.6410961852073669 accuracy: 82.59\n",
      " batch 6000 lost: 1.6491162472963332 accuracy: 82.37083333333334\n",
      " batch 7000 lost: 1.6444159115552903 accuracy: 82.29642857142858\n",
      " batch 8000 lost: 1.6365468685626983 accuracy: 82.328125\n",
      " batch 9000 lost: 1.6392032195329667 accuracy: 82.33611111111111\n",
      " batch 10000 lost: 1.637852069735527 accuracy: 82.3375\n",
      " batch 11000 lost: 1.6434812310934066 accuracy: 82.2909090909091\n",
      " batch 12000 lost: 1.6363773684501648 accuracy: 82.32708333333333\n",
      " batch 13000 lost: 1.6503290145397187 accuracy: 82.23846153846154\n",
      " batch 14000 lost: 1.6379973464012145 accuracy: 82.24464285714286\n",
      " batch 15000 lost: 1.6405438870191573 accuracy: 82.24\n",
      "LOSS train 1.6405438870191573, valid 1.654039978981018\n",
      "ACCURACY train 82.24, valid 80.71\n",
      "EPOCH 7:\n",
      " batch 1000 lost: 1.6422595341205597 accuracy: 81.9\n",
      " batch 2000 lost: 1.627742581129074 accuracy: 82.6875\n",
      " batch 3000 lost: 1.639829824924469 accuracy: 82.54166666666667\n",
      " batch 4000 lost: 1.64434301841259 accuracy: 82.375\n",
      " batch 5000 lost: 1.6401313494443894 accuracy: 82.28\n",
      " batch 6000 lost: 1.6428438630104065 accuracy: 82.24166666666666\n",
      " batch 7000 lost: 1.6460689841508864 accuracy: 82.16428571428571\n",
      " batch 8000 lost: 1.6405973302125931 accuracy: 82.165625\n",
      " batch 9000 lost: 1.6311225671768188 accuracy: 82.26944444444445\n",
      " batch 10000 lost: 1.644180983543396 accuracy: 82.2125\n",
      " batch 11000 lost: 1.6256450945138932 accuracy: 82.34318181818182\n",
      " batch 12000 lost: 1.6362089482545852 accuracy: 82.36875\n",
      " batch 13000 lost: 1.6359474202394486 accuracy: 82.37115384615385\n",
      " batch 14000 lost: 1.6398497415781021 accuracy: 82.36607142857143\n",
      " batch 15000 lost: 1.6284665304422379 accuracy: 82.43166666666667\n",
      "LOSS train 1.6284665304422379, valid 1.645704984664917\n",
      "ACCURACY train 82.43166666666667, valid 81.57\n",
      "EPOCH 8:\n",
      " batch 1000 lost: 1.6387644064426423 accuracy: 82.25\n",
      " batch 2000 lost: 1.6310545642375947 accuracy: 82.7375\n",
      " batch 3000 lost: 1.6421724070310593 accuracy: 82.48333333333333\n",
      " batch 4000 lost: 1.6302954465150834 accuracy: 82.61875\n",
      " batch 5000 lost: 1.633963061928749 accuracy: 82.615\n",
      " batch 6000 lost: 1.6468183182477951 accuracy: 82.40416666666667\n",
      " batch 7000 lost: 1.623735307097435 accuracy: 82.6\n",
      " batch 8000 lost: 1.6331414605379104 accuracy: 82.625\n",
      " batch 9000 lost: 1.6348477231264114 accuracy: 82.63055555555556\n",
      " batch 10000 lost: 1.6245281468629837 accuracy: 82.7375\n",
      " batch 11000 lost: 1.638291798233986 accuracy: 82.70681818181818\n",
      " batch 12000 lost: 1.6402563058137893 accuracy: 82.65208333333334\n",
      " batch 13000 lost: 1.6322397760152816 accuracy: 82.66923076923077\n",
      " batch 14000 lost: 1.6373782522678375 accuracy: 82.65892857142858\n",
      " batch 15000 lost: 1.6313710225820541 accuracy: 82.68\n",
      "LOSS train 1.6313710225820541, valid 1.6527780294418335\n",
      "ACCURACY train 82.68, valid 80.88\n",
      "EPOCH 9:\n",
      " batch 1000 lost: 1.6299165710210801 accuracy: 83.175\n",
      " batch 2000 lost: 1.6402116091251373 accuracy: 82.675\n",
      " batch 3000 lost: 1.6350556381940842 accuracy: 82.74166666666666\n",
      " batch 4000 lost: 1.6289680151939392 accuracy: 82.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 5000 lost: 1.6334683166742325 accuracy: 82.905\n",
      " batch 6000 lost: 1.6314189043045044 accuracy: 82.94166666666666\n",
      " batch 7000 lost: 1.6276137920618057 accuracy: 82.99285714285715\n",
      " batch 8000 lost: 1.6289863377809524 accuracy: 83.04375\n",
      " batch 9000 lost: 1.625151030898094 accuracy: 83.125\n",
      " batch 10000 lost: 1.6255769392251969 accuracy: 83.1725\n",
      " batch 11000 lost: 1.6371475266218185 accuracy: 83.10909090909091\n",
      " batch 12000 lost: 1.6381079621315002 accuracy: 83.03958333333334\n",
      " batch 13000 lost: 1.6419566210508347 accuracy: 82.95192307692308\n",
      " batch 14000 lost: 1.631481862783432 accuracy: 82.95714285714286\n",
      " batch 15000 lost: 1.6315387135744095 accuracy: 82.97166666666666\n",
      "LOSS train 1.6315387135744095, valid 1.6418797969818115\n",
      "ACCURACY train 82.97166666666666, valid 81.94\n",
      "EPOCH 10:\n",
      " batch 1000 lost: 1.636652727007866 accuracy: 82.575\n",
      " batch 2000 lost: 1.6206487760543824 accuracy: 83.3375\n",
      " batch 3000 lost: 1.6387861504554748 accuracy: 82.98333333333333\n",
      " batch 4000 lost: 1.6287572388648988 accuracy: 83.06875\n",
      " batch 5000 lost: 1.6334230284690856 accuracy: 83.02\n",
      " batch 6000 lost: 1.628591426372528 accuracy: 83.04166666666667\n",
      " batch 7000 lost: 1.6427087134122849 accuracy: 82.86785714285715\n",
      " batch 8000 lost: 1.624943845629692 accuracy: 82.96875\n",
      " batch 9000 lost: 1.6307080272436143 accuracy: 82.98333333333333\n",
      " batch 10000 lost: 1.6300552390813827 accuracy: 82.9925\n",
      " batch 11000 lost: 1.6407095435857773 accuracy: 82.91136363636363\n",
      " batch 12000 lost: 1.6327796449661256 accuracy: 82.9125\n",
      " batch 13000 lost: 1.632266655921936 accuracy: 82.91730769230769\n",
      " batch 14000 lost: 1.6220109233856201 accuracy: 82.98928571428571\n",
      " batch 15000 lost: 1.6172270513772964 accuracy: 83.08833333333334\n",
      "LOSS train 1.6172270513772964, valid 1.642651081085205\n",
      "ACCURACY train 83.08833333333334, valid 81.86\n",
      "EPOCH 11:\n",
      " batch 1000 lost: 1.6297241785526275 accuracy: 83.15\n",
      " batch 2000 lost: 1.6306078503131867 accuracy: 83.1875\n",
      " batch 3000 lost: 1.6291396436691283 accuracy: 83.225\n",
      " batch 4000 lost: 1.6339278061389924 accuracy: 83.11875\n",
      " batch 5000 lost: 1.6320715503692627 accuracy: 83.08\n",
      " batch 6000 lost: 1.6238954792022704 accuracy: 83.19166666666666\n",
      " batch 7000 lost: 1.6313256583213807 accuracy: 83.15357142857142\n",
      " batch 8000 lost: 1.6290119256973266 accuracy: 83.16875\n",
      " batch 9000 lost: 1.6278859528303147 accuracy: 83.18888888888888\n",
      " batch 10000 lost: 1.630379896402359 accuracy: 83.175\n",
      " batch 11000 lost: 1.6244914162158965 accuracy: 83.22954545454546\n",
      " batch 12000 lost: 1.6213589953184129 accuracy: 83.30208333333333\n",
      " batch 13000 lost: 1.6285448917150498 accuracy: 83.29807692307692\n",
      " batch 14000 lost: 1.6318061698675155 accuracy: 83.26785714285714\n",
      " batch 15000 lost: 1.6274740660190583 accuracy: 83.27166666666666\n",
      "LOSS train 1.6274740660190583, valid 1.643520474433899\n",
      "ACCURACY train 83.27166666666666, valid 81.66\n",
      "EPOCH 12:\n",
      " batch 1000 lost: 1.6343005337715149 accuracy: 82.825\n",
      " batch 2000 lost: 1.6271469330787658 accuracy: 83.1375\n",
      " batch 3000 lost: 1.6210028665065765 accuracy: 83.46666666666667\n",
      " batch 4000 lost: 1.6347446318864822 accuracy: 83.26875\n",
      " batch 5000 lost: 1.6229199501276017 accuracy: 83.4\n",
      " batch 6000 lost: 1.6251789014339446 accuracy: 83.46666666666667\n",
      " batch 7000 lost: 1.6288448066711425 accuracy: 83.45357142857142\n",
      " batch 8000 lost: 1.633991889834404 accuracy: 83.359375\n",
      " batch 9000 lost: 1.6236100714206696 accuracy: 83.41944444444445\n",
      " batch 10000 lost: 1.6319535013437272 accuracy: 83.3925\n",
      " batch 11000 lost: 1.6155062341690063 accuracy: 83.51136363636364\n",
      " batch 12000 lost: 1.6239460886716843 accuracy: 83.5375\n",
      " batch 13000 lost: 1.6242922707796097 accuracy: 83.53846153846153\n",
      " batch 14000 lost: 1.6353085500001907 accuracy: 83.47321428571429\n",
      " batch 15000 lost: 1.628375700712204 accuracy: 83.46333333333334\n",
      "LOSS train 1.628375700712204, valid 1.6381810903549194\n",
      "ACCURACY train 83.46333333333334, valid 82.36\n",
      "EPOCH 13:\n",
      " batch 1000 lost: 1.6235460770130157 accuracy: 83.675\n",
      " batch 2000 lost: 1.6245709154605865 accuracy: 83.65\n",
      " batch 3000 lost: 1.6301468193531037 accuracy: 83.45833333333333\n",
      " batch 4000 lost: 1.627877576112747 accuracy: 83.41875\n",
      " batch 5000 lost: 1.6206472134590149 accuracy: 83.57\n",
      " batch 6000 lost: 1.619752420783043 accuracy: 83.7\n",
      " batch 7000 lost: 1.6283167390823363 accuracy: 83.65714285714286\n",
      " batch 8000 lost: 1.6266019634008408 accuracy: 83.646875\n",
      " batch 9000 lost: 1.6258945667743683 accuracy: 83.63888888888889\n",
      " batch 10000 lost: 1.6293856987953186 accuracy: 83.6\n",
      " batch 11000 lost: 1.6263268215656281 accuracy: 83.5909090909091\n",
      " batch 12000 lost: 1.6303257486820222 accuracy: 83.55833333333334\n",
      " batch 13000 lost: 1.6234886426925659 accuracy: 83.56923076923077\n",
      " batch 14000 lost: 1.6282713457345963 accuracy: 83.5625\n",
      " batch 15000 lost: 1.614242414355278 accuracy: 83.64833333333333\n",
      "LOSS train 1.614242414355278, valid 1.6389433145523071\n",
      "ACCURACY train 83.64833333333333, valid 82.29\n",
      "EPOCH 14:\n",
      " batch 1000 lost: 1.633642780303955 accuracy: 82.775\n",
      " batch 2000 lost: 1.6263683352470397 accuracy: 83.1125\n",
      " batch 3000 lost: 1.6257821646928787 accuracy: 83.30833333333334\n",
      " batch 4000 lost: 1.626066346526146 accuracy: 83.40625\n",
      " batch 5000 lost: 1.629980558156967 accuracy: 83.365\n",
      " batch 6000 lost: 1.6261943446397782 accuracy: 83.4\n",
      " batch 7000 lost: 1.6231400767564774 accuracy: 83.45714285714286\n",
      " batch 8000 lost: 1.6317102756500244 accuracy: 83.375\n",
      " batch 9000 lost: 1.623938788175583 accuracy: 83.41666666666667\n",
      " batch 10000 lost: 1.607892795920372 accuracy: 83.6175\n",
      " batch 11000 lost: 1.6216092014312744 accuracy: 83.66818181818182\n",
      " batch 12000 lost: 1.62585093665123 accuracy: 83.6625\n",
      " batch 13000 lost: 1.6233605720996858 accuracy: 83.6826923076923\n",
      " batch 14000 lost: 1.6125877178907395 accuracy: 83.77321428571429\n",
      " batch 15000 lost: 1.6279027203321457 accuracy: 83.75\n",
      "LOSS train 1.6279027203321457, valid 1.6406728029251099\n",
      "ACCURACY train 83.75, valid 81.95\n",
      "EPOCH 15:\n",
      " batch 1000 lost: 1.6298568531274795 accuracy: 83.225\n",
      " batch 2000 lost: 1.6225378766059875 accuracy: 83.5875\n",
      " batch 3000 lost: 1.6194942673444748 accuracy: 83.80833333333334\n",
      " batch 4000 lost: 1.6205508782863618 accuracy: 83.9\n",
      " batch 5000 lost: 1.6285392464399338 accuracy: 83.775\n",
      " batch 6000 lost: 1.6248207651376725 accuracy: 83.76666666666667\n",
      " batch 7000 lost: 1.6272998843193054 accuracy: 83.71071428571429\n",
      " batch 8000 lost: 1.6136837166547775 accuracy: 83.821875\n",
      " batch 9000 lost: 1.6291633667945862 accuracy: 83.74722222222222\n",
      " batch 10000 lost: 1.626137300491333 accuracy: 83.7225\n",
      " batch 11000 lost: 1.6169313100576401 accuracy: 83.79772727272727\n",
      " batch 12000 lost: 1.6225880521535874 accuracy: 83.80833333333334\n",
      " batch 13000 lost: 1.6215455560684204 accuracy: 83.82307692307693\n",
      " batch 14000 lost: 1.6218584275245667 accuracy: 83.82857142857142\n",
      " batch 15000 lost: 1.6108996274471283 accuracy: 83.89833333333333\n",
      "LOSS train 1.6108996274471283, valid 1.6351172924041748\n",
      "ACCURACY train 83.89833333333333, valid 82.58\n",
      "EPOCH 16:\n",
      " batch 1000 lost: 1.6134099708795548 accuracy: 84.85\n",
      " batch 2000 lost: 1.6284817975759507 accuracy: 84.1\n",
      " batch 3000 lost: 1.6078291745185853 accuracy: 84.53333333333333\n",
      " batch 4000 lost: 1.6230040991306305 accuracy: 84.34375\n",
      " batch 5000 lost: 1.624270367383957 accuracy: 84.195\n",
      " batch 6000 lost: 1.6334426248073577 accuracy: 83.97083333333333\n",
      " batch 7000 lost: 1.6180824078321456 accuracy: 84.025\n",
      " batch 8000 lost: 1.618946298956871 accuracy: 84.046875\n",
      " batch 9000 lost: 1.6236556997299194 accuracy: 84.01388888888889\n",
      " batch 10000 lost: 1.6227617902755738 accuracy: 84.005\n",
      " batch 11000 lost: 1.6234349201917648 accuracy: 83.98636363636363\n",
      " batch 12000 lost: 1.616884863138199 accuracy: 84.025\n",
      " batch 13000 lost: 1.620259633421898 accuracy: 84.03461538461538\n",
      " batch 14000 lost: 1.630512148141861 accuracy: 83.96071428571429\n",
      " batch 15000 lost: 1.6176038545370102 accuracy: 83.98833333333333\n",
      "LOSS train 1.6176038545370102, valid 1.6512120962142944\n",
      "ACCURACY train 83.98833333333333, valid 80.95\n",
      "EPOCH 17:\n",
      " batch 1000 lost: 1.6190238324403763 accuracy: 84.225\n",
      " batch 2000 lost: 1.6097229408025742 accuracy: 84.6875\n",
      " batch 3000 lost: 1.6237090400457381 accuracy: 84.4\n",
      " batch 4000 lost: 1.6102944926023484 accuracy: 84.61875\n",
      " batch 5000 lost: 1.6186235815286636 accuracy: 84.555\n",
      " batch 6000 lost: 1.625150541782379 accuracy: 84.40833333333333\n",
      " batch 7000 lost: 1.6278984625339508 accuracy: 84.24642857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 8000 lost: 1.6358435410261154 accuracy: 84.021875\n",
      " batch 9000 lost: 1.6197460980415344 accuracy: 84.04722222222222\n",
      " batch 10000 lost: 1.624627382516861 accuracy: 84.0075\n",
      " batch 11000 lost: 1.6114849928617478 accuracy: 84.09772727272727\n",
      " batch 12000 lost: 1.6166605832576753 accuracy: 84.14166666666667\n",
      " batch 13000 lost: 1.6259600692987441 accuracy: 84.09423076923076\n",
      " batch 14000 lost: 1.6233221871852874 accuracy: 84.08035714285714\n",
      " batch 15000 lost: 1.6195814930200576 accuracy: 84.09\n",
      "LOSS train 1.6195814930200576, valid 1.6363716125488281\n",
      "ACCURACY train 84.09, valid 82.48\n",
      "EPOCH 18:\n",
      " batch 1000 lost: 1.6271648434400559 accuracy: 83.475\n",
      " batch 2000 lost: 1.6343629977703094 accuracy: 83.025\n",
      " batch 3000 lost: 1.609415134191513 accuracy: 83.7\n",
      " batch 4000 lost: 1.6198739347457887 accuracy: 83.78125\n",
      " batch 5000 lost: 1.6000914902687073 accuracy: 84.27\n",
      " batch 6000 lost: 1.6082959698438644 accuracy: 84.45833333333333\n",
      " batch 7000 lost: 1.6124256856441497 accuracy: 84.52142857142857\n",
      " batch 8000 lost: 1.6182363860607147 accuracy: 84.503125\n",
      " batch 9000 lost: 1.6198233176469803 accuracy: 84.46111111111111\n",
      " batch 10000 lost: 1.628979876756668 accuracy: 84.345\n",
      " batch 11000 lost: 1.6238118072748184 accuracy: 84.28636363636363\n",
      " batch 12000 lost: 1.6210835399627685 accuracy: 84.27083333333333\n",
      " batch 13000 lost: 1.6258871426582338 accuracy: 84.20769230769231\n",
      " batch 14000 lost: 1.6239771176576614 accuracy: 84.17857142857143\n",
      " batch 15000 lost: 1.6215744290351868 accuracy: 84.16666666666667\n",
      "LOSS train 1.6215744290351868, valid 1.6337625980377197\n",
      "ACCURACY train 84.16666666666667, valid 82.7\n",
      "EPOCH 19:\n",
      " batch 1000 lost: 1.616480422616005 accuracy: 84.5\n",
      " batch 2000 lost: 1.615948118209839 accuracy: 84.5375\n",
      " batch 3000 lost: 1.6118391246795654 accuracy: 84.65833333333333\n",
      " batch 4000 lost: 1.6149236398935318 accuracy: 84.675\n",
      " batch 5000 lost: 1.617405006289482 accuracy: 84.62\n",
      " batch 6000 lost: 1.6207560822963714 accuracy: 84.51666666666667\n",
      " batch 7000 lost: 1.6047144669294358 accuracy: 84.69285714285714\n",
      " batch 8000 lost: 1.6197534973621368 accuracy: 84.6375\n",
      " batch 9000 lost: 1.622619380235672 accuracy: 84.56666666666666\n",
      " batch 10000 lost: 1.6200586866140365 accuracy: 84.5275\n",
      " batch 11000 lost: 1.6212012158632279 accuracy: 84.47727272727273\n",
      " batch 12000 lost: 1.6070215120315552 accuracy: 84.56458333333333\n",
      " batch 13000 lost: 1.620572329878807 accuracy: 84.52692307692308\n",
      " batch 14000 lost: 1.6251387014389038 accuracy: 84.45178571428572\n",
      " batch 15000 lost: 1.6283301352262496 accuracy: 84.37166666666667\n",
      "LOSS train 1.6283301352262496, valid 1.633633017539978\n",
      "ACCURACY train 84.37166666666667, valid 82.8\n",
      "EPOCH 20:\n",
      " batch 1000 lost: 1.6151204961538315 accuracy: 84.575\n",
      " batch 2000 lost: 1.6087328425645828 accuracy: 85.0\n",
      " batch 3000 lost: 1.6268438986539842 accuracy: 84.46666666666667\n",
      " batch 4000 lost: 1.6180374623537064 accuracy: 84.4375\n",
      " batch 5000 lost: 1.6268639212846756 accuracy: 84.25\n",
      " batch 6000 lost: 1.6141752893924712 accuracy: 84.32916666666667\n",
      " batch 7000 lost: 1.6033336794376374 accuracy: 84.54642857142858\n",
      " batch 8000 lost: 1.627002812743187 accuracy: 84.4125\n",
      " batch 9000 lost: 1.615872334241867 accuracy: 84.425\n",
      " batch 10000 lost: 1.617110340833664 accuracy: 84.4225\n",
      " batch 11000 lost: 1.6115376420021057 accuracy: 84.47954545454546\n",
      " batch 12000 lost: 1.628229756116867 accuracy: 84.37291666666667\n",
      " batch 13000 lost: 1.6079314466714858 accuracy: 84.44615384615385\n",
      " batch 14000 lost: 1.619785637974739 accuracy: 84.42142857142858\n",
      " batch 15000 lost: 1.610303420305252 accuracy: 84.47\n",
      "LOSS train 1.610303420305252, valid 1.6349903345108032\n",
      "ACCURACY train 84.47, valid 82.51\n",
      "EPOCH 21:\n",
      " batch 1000 lost: 1.6198026436567305 accuracy: 84.15\n",
      " batch 2000 lost: 1.608936726808548 accuracy: 84.7625\n",
      " batch 3000 lost: 1.6315416259765625 accuracy: 84.14166666666667\n",
      " batch 4000 lost: 1.6170338561534883 accuracy: 84.2375\n",
      " batch 5000 lost: 1.623660143971443 accuracy: 84.14\n",
      " batch 6000 lost: 1.6176600263118743 accuracy: 84.19166666666666\n",
      " batch 7000 lost: 1.6090930368900298 accuracy: 84.34285714285714\n",
      " batch 8000 lost: 1.6057950699329375 accuracy: 84.503125\n",
      " batch 9000 lost: 1.6112669590711595 accuracy: 84.55833333333334\n",
      " batch 10000 lost: 1.6208034707307815 accuracy: 84.5075\n",
      " batch 11000 lost: 1.6110949535369874 accuracy: 84.56136363636364\n",
      " batch 12000 lost: 1.6115580693483353 accuracy: 84.58333333333333\n",
      " batch 13000 lost: 1.615829886674881 accuracy: 84.58269230769231\n",
      " batch 14000 lost: 1.6183107607364655 accuracy: 84.56785714285714\n",
      " batch 15000 lost: 1.6151897110939026 accuracy: 84.57666666666667\n",
      "LOSS train 1.6151897110939026, valid 1.6351659297943115\n",
      "ACCURACY train 84.57666666666667, valid 82.55\n",
      "EPOCH 22:\n",
      " batch 1000 lost: 1.6108716504573821 accuracy: 85.0\n",
      " batch 2000 lost: 1.6141480073928833 accuracy: 84.8625\n",
      " batch 3000 lost: 1.6182851123809814 accuracy: 84.68333333333334\n",
      " batch 4000 lost: 1.6103830538988113 accuracy: 84.7875\n",
      " batch 5000 lost: 1.6024802471399306 accuracy: 84.985\n",
      " batch 6000 lost: 1.6118256367444992 accuracy: 84.9875\n",
      " batch 7000 lost: 1.6149645804166795 accuracy: 84.93571428571428\n",
      " batch 8000 lost: 1.619617809534073 accuracy: 84.84375\n",
      " batch 9000 lost: 1.617666788458824 accuracy: 84.79444444444445\n",
      " batch 10000 lost: 1.6214820553064346 accuracy: 84.7125\n",
      " batch 11000 lost: 1.6134288675785065 accuracy: 84.72045454545454\n",
      " batch 12000 lost: 1.6095932021141053 accuracy: 84.7625\n",
      " batch 13000 lost: 1.6218019666671752 accuracy: 84.70576923076923\n",
      " batch 14000 lost: 1.6192227299213409 accuracy: 84.67678571428571\n",
      " batch 15000 lost: 1.6173258913755417 accuracy: 84.66833333333334\n",
      "LOSS train 1.6173258913755417, valid 1.6333222389221191\n",
      "ACCURACY train 84.66833333333334, valid 82.75\n",
      "EPOCH 23:\n",
      " batch 1000 lost: 1.617556135892868 accuracy: 84.425\n",
      " batch 2000 lost: 1.6113573365211487 accuracy: 84.7\n",
      " batch 3000 lost: 1.621481879234314 accuracy: 84.475\n",
      " batch 4000 lost: 1.6073570977449416 accuracy: 84.7375\n",
      " batch 5000 lost: 1.620388806581497 accuracy: 84.605\n",
      " batch 6000 lost: 1.6118649357557298 accuracy: 84.65416666666667\n",
      " batch 7000 lost: 1.6166947187185288 accuracy: 84.62857142857143\n",
      " batch 8000 lost: 1.603747066140175 accuracy: 84.771875\n",
      " batch 9000 lost: 1.6144117727279663 accuracy: 84.7611111111111\n",
      " batch 10000 lost: 1.6108228986263275 accuracy: 84.8\n",
      " batch 11000 lost: 1.6154325575828552 accuracy: 84.76818181818182\n",
      " batch 12000 lost: 1.6243959914445878 accuracy: 84.67916666666666\n",
      " batch 13000 lost: 1.6141074500083923 accuracy: 84.68846153846154\n",
      " batch 14000 lost: 1.6122425303459167 accuracy: 84.71428571428571\n",
      " batch 15000 lost: 1.6126463514566423 accuracy: 84.72833333333334\n",
      "LOSS train 1.6126463514566423, valid 1.6420780420303345\n",
      "ACCURACY train 84.72833333333334, valid 81.82\n",
      "EPOCH 24:\n",
      " batch 1000 lost: 1.6153284482955932 accuracy: 84.575\n",
      " batch 2000 lost: 1.610757416844368 accuracy: 84.8625\n",
      " batch 3000 lost: 1.6166822443008422 accuracy: 84.725\n",
      " batch 4000 lost: 1.6082180700302124 accuracy: 84.85625\n",
      " batch 5000 lost: 1.6130539059638977 accuracy: 84.84\n",
      " batch 6000 lost: 1.6117533267736435 accuracy: 84.86666666666666\n",
      " batch 7000 lost: 1.6087297534942626 accuracy: 84.92142857142858\n",
      " batch 8000 lost: 1.6036344509124756 accuracy: 85.040625\n",
      " batch 9000 lost: 1.6151226143836974 accuracy: 85.01388888888889\n",
      " batch 10000 lost: 1.6103487020730973 accuracy: 85.0225\n",
      " batch 11000 lost: 1.6200842467546464 accuracy: 84.95\n",
      " batch 12000 lost: 1.6138051207065582 accuracy: 84.94375\n",
      " batch 13000 lost: 1.619416764974594 accuracy: 84.88653846153846\n",
      " batch 14000 lost: 1.6151783930063248 accuracy: 84.86785714285715\n",
      " batch 15000 lost: 1.6173602038621901 accuracy: 84.84166666666667\n",
      "LOSS train 1.6173602038621901, valid 1.6338855028152466\n",
      "ACCURACY train 84.84166666666667, valid 82.65\n",
      "EPOCH 25:\n",
      " batch 1000 lost: 1.6188242042064667 accuracy: 84.25\n",
      " batch 2000 lost: 1.6108468625545502 accuracy: 84.6\n",
      " batch 3000 lost: 1.6050915681123734 accuracy: 84.94166666666666\n",
      " batch 4000 lost: 1.6119812527894974 accuracy: 84.91875\n",
      " batch 5000 lost: 1.6302821671962737 accuracy: 84.56\n",
      " batch 6000 lost: 1.6214854918718338 accuracy: 84.4625\n",
      " batch 7000 lost: 1.6109517232179642 accuracy: 84.54285714285714\n",
      " batch 8000 lost: 1.6124866745471955 accuracy: 84.596875\n",
      " batch 9000 lost: 1.6204896911382676 accuracy: 84.53888888888889\n",
      " batch 10000 lost: 1.6138751729726792 accuracy: 84.5575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 11000 lost: 1.603282165288925 accuracy: 84.68636363636364\n",
      " batch 12000 lost: 1.6108240818977355 accuracy: 84.70833333333333\n",
      " batch 13000 lost: 1.6033366725444793 accuracy: 84.80192307692307\n",
      " batch 14000 lost: 1.6117763985395432 accuracy: 84.81964285714285\n",
      " batch 15000 lost: 1.6084356726408005 accuracy: 84.86\n",
      "LOSS train 1.6084356726408005, valid 1.6313750743865967\n",
      "ACCURACY train 84.86, valid 82.93\n",
      "EPOCH 26:\n",
      " batch 1000 lost: 1.6047080523967743 accuracy: 85.675\n",
      " batch 2000 lost: 1.6154096392393111 accuracy: 85.1\n",
      " batch 3000 lost: 1.6023183507919312 accuracy: 85.39166666666667\n",
      " batch 4000 lost: 1.6114462589025498 accuracy: 85.3\n",
      " batch 5000 lost: 1.6168711190223695 accuracy: 85.13\n",
      " batch 6000 lost: 1.6009796509742737 accuracy: 85.3\n",
      " batch 7000 lost: 1.608482966899872 accuracy: 85.30357142857143\n",
      " batch 8000 lost: 1.6049316502809525 accuracy: 85.35\n",
      " batch 9000 lost: 1.6011674646139145 accuracy: 85.43055555555556\n",
      " batch 10000 lost: 1.6154028750658036 accuracy: 85.3425\n",
      " batch 11000 lost: 1.6211758667230607 accuracy: 85.22727272727273\n",
      " batch 12000 lost: 1.6185508863925935 accuracy: 85.15416666666667\n",
      " batch 13000 lost: 1.6174641387462616 accuracy: 85.08269230769231\n",
      " batch 14000 lost: 1.620284332036972 accuracy: 85.01964285714286\n",
      " batch 15000 lost: 1.61753823697567 accuracy: 84.98166666666667\n",
      "LOSS train 1.61753823697567, valid 1.6366397142410278\n",
      "ACCURACY train 84.98166666666667, valid 82.43\n",
      "EPOCH 27:\n",
      " batch 1000 lost: 1.6174180278778076 accuracy: 84.375\n",
      " batch 2000 lost: 1.6060747487545013 accuracy: 84.9625\n",
      " batch 3000 lost: 1.6104713340997696 accuracy: 85.04166666666667\n",
      " batch 4000 lost: 1.605582504630089 accuracy: 85.175\n",
      " batch 5000 lost: 1.6100305715799332 accuracy: 85.16\n",
      " batch 6000 lost: 1.6142427304983138 accuracy: 85.09583333333333\n",
      " batch 7000 lost: 1.6162751759290694 accuracy: 85.01428571428572\n",
      " batch 8000 lost: 1.6172138205766677 accuracy: 84.925\n",
      " batch 9000 lost: 1.618265925526619 accuracy: 84.86666666666666\n",
      " batch 10000 lost: 1.6085417456626891 accuracy: 84.9075\n",
      " batch 11000 lost: 1.6124287594556808 accuracy: 84.90227272727273\n",
      " batch 12000 lost: 1.6008869197368623 accuracy: 85.00208333333333\n",
      " batch 13000 lost: 1.6106898134946823 accuracy: 85.00769230769231\n",
      " batch 14000 lost: 1.6122646774053573 accuracy: 85.00535714285714\n",
      " batch 15000 lost: 1.6057796487808227 accuracy: 85.05\n",
      "LOSS train 1.6057796487808227, valid 1.6304607391357422\n",
      "ACCURACY train 85.05, valid 83.13\n",
      "EPOCH 28:\n",
      " batch 1000 lost: 1.6166086673736573 accuracy: 84.6\n",
      " batch 2000 lost: 1.607939321756363 accuracy: 85.0375\n",
      " batch 3000 lost: 1.6030077252388 accuracy: 85.325\n",
      " batch 4000 lost: 1.621345995426178 accuracy: 84.9875\n",
      " batch 5000 lost: 1.6135512684583664 accuracy: 84.955\n",
      " batch 6000 lost: 1.6109576869010924 accuracy: 84.96666666666667\n",
      " batch 7000 lost: 1.6115374706983567 accuracy: 84.975\n",
      " batch 8000 lost: 1.6041357270479202 accuracy: 85.084375\n",
      " batch 9000 lost: 1.6122561601400376 accuracy: 85.07777777777778\n",
      " batch 10000 lost: 1.5978839337825774 accuracy: 85.2\n",
      " batch 11000 lost: 1.6168020949363708 accuracy: 85.13863636363637\n",
      " batch 12000 lost: 1.6144665768146516 accuracy: 85.09583333333333\n",
      " batch 13000 lost: 1.6056497066020965 accuracy: 85.12692307692308\n",
      " batch 14000 lost: 1.6073318861722947 accuracy: 85.15\n",
      " batch 15000 lost: 1.606975400328636 accuracy: 85.17\n",
      "LOSS train 1.606975400328636, valid 1.6338741779327393\n",
      "ACCURACY train 85.17, valid 82.67\n",
      "EPOCH 29:\n",
      " batch 1000 lost: 1.610775653719902 accuracy: 85.025\n",
      " batch 2000 lost: 1.618675714492798 accuracy: 84.65\n",
      " batch 3000 lost: 1.6001718760728836 accuracy: 85.14166666666667\n",
      " batch 4000 lost: 1.606318262219429 accuracy: 85.20625\n",
      " batch 5000 lost: 1.6044033967256546 accuracy: 85.285\n",
      " batch 6000 lost: 1.617399036049843 accuracy: 85.13333333333334\n",
      " batch 7000 lost: 1.608676558971405 accuracy: 85.14285714285714\n",
      " batch 8000 lost: 1.6106399530172348 accuracy: 85.121875\n",
      " batch 9000 lost: 1.6068790789842606 accuracy: 85.16388888888889\n",
      " batch 10000 lost: 1.613222228050232 accuracy: 85.1325\n",
      " batch 11000 lost: 1.6026812106370927 accuracy: 85.19318181818181\n",
      " batch 12000 lost: 1.6132949191331862 accuracy: 85.16666666666667\n",
      " batch 13000 lost: 1.6104148273468017 accuracy: 85.15192307692308\n",
      " batch 14000 lost: 1.615600478053093 accuracy: 85.10714285714286\n",
      " batch 15000 lost: 1.6110303242206574 accuracy: 85.10833333333333\n",
      "LOSS train 1.6110303242206574, valid 1.6368058919906616\n",
      "ACCURACY train 85.10833333333333, valid 82.31\n",
      "EPOCH 30:\n",
      " batch 1000 lost: 1.6135371193885804 accuracy: 84.85\n",
      " batch 2000 lost: 1.6024591472148895 accuracy: 85.375\n",
      " batch 3000 lost: 1.6103214727640152 accuracy: 85.28333333333333\n",
      " batch 4000 lost: 1.6104550856351851 accuracy: 85.225\n",
      " batch 5000 lost: 1.6081396028995514 accuracy: 85.23\n",
      " batch 6000 lost: 1.6164423851966858 accuracy: 85.10833333333333\n",
      " batch 7000 lost: 1.6120044889450074 accuracy: 85.09285714285714\n",
      " batch 8000 lost: 1.6027056008577347 accuracy: 85.184375\n",
      " batch 9000 lost: 1.6088832548856735 accuracy: 85.19166666666666\n",
      " batch 10000 lost: 1.6145071519613265 accuracy: 85.135\n",
      " batch 11000 lost: 1.6186832869052887 accuracy: 85.05681818181819\n",
      " batch 12000 lost: 1.605578889966011 accuracy: 85.10208333333334\n",
      " batch 13000 lost: 1.6132453683614731 accuracy: 85.08269230769231\n",
      " batch 14000 lost: 1.6041951429843904 accuracy: 85.13392857142857\n",
      " batch 15000 lost: 1.5982852151393891 accuracy: 85.22\n",
      "LOSS train 1.5982852151393891, valid 1.6305261850357056\n",
      "ACCURACY train 85.22, valid 83.04\n"
     ]
    }
   ],
   "source": [
    "# Actual training loop: Equivalent to tensorflow.fit\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "best_vloss = 1_000_000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch+1}:')\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, accuracy = one_epoch_training(epoch, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    # Set the model to evaluation mode, disabling dropout and using population statistics for batch normalization\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption\n",
    "    # Calculate validation loss here\n",
    "    with torch.no_grad():\n",
    "        vcorrect = 0\n",
    "        vsamples = 0\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            vpredictions = model(vinputs)\n",
    "            _, vpredict = torch.max(vpredictions, 1)\n",
    "            vloss = loss_fn(vpredictions, vlabels)\n",
    "            running_vloss += vloss\n",
    "            vcorrect += (vpredict == vlabels).sum().item()\n",
    "            vsamples += vlabels.size(0)\n",
    "\n",
    "    vaccuracy = 100 * vcorrect / vsamples\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train {avg_loss}, valid {avg_vloss}')\n",
    "    print(f'ACCURACY train {accuracy}, valid {vaccuracy}')\n",
    "\n",
    "    writer.add_scalars('Training vs. Validation Loss', {\n",
    "        'Training' : avg_loss, 'Validation' : avg_vloss\n",
    "    }, epoch + 1)\n",
    "    writer.add_scalars('Training vs. Validation Accuracy', {\n",
    "        'Training' : accuracy, 'Validation' : vaccuracy\n",
    "    }, epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the best model state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f'Models/FashionMNIST/fashion_model_{timestamp}_{epoch}'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043c097-0e46-440e-bee2-0fc278162e15",
   "metadata": {},
   "source": [
    "use tensorboard --lodir=\"runs/\" to view accuracy and loss graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce17ac-7676-4d27-8127-ef0c9708ee6b",
   "metadata": {},
   "source": [
    "Overall the accuracy is similar to the TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62d37536-1ce3-47e5-98da-5d313b944551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11610, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch california house data\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using previous housing to be consistent with thje book\n",
    "housing = fetch_california_housing()\n",
    "cal_x_train_full, cal_x_test, cal_y_train_full, cal_y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "cal_x_train, cal_x_val, cal_y_train, cal_y_val = train_test_split(\n",
    "    cal_x_train_full, cal_y_train_full, random_state=42)\n",
    "\n",
    "cal_x_train = torch.Tensor(cal_x_train)\n",
    "cal_y_train = torch.Tensor(cal_y_train)\n",
    "cal_x_test = torch.Tensor(cal_x_test)\n",
    "cal_y_test = torch.Tensor(cal_y_test)\n",
    "cal_x_val = torch.Tensor(cal_x_val)\n",
    "cal_y_val = torch.Tensor(cal_y_val)\n",
    "cal_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6e03968-34f9-42ee-a354-782bed44e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels# Make CustomDataSet\n",
    "\n",
    "class CalHousingDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, values):\n",
    "        self.features = features\n",
    "        self.values = values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        feature = (feature - feature.mean()) / feature.std()\n",
    "        value = self.values[index]\n",
    "        return feature, value\n",
    "        \n",
    "cal_training = CalHousingDataSet(cal_x_train, cal_y_train)\n",
    "cal_validation = CalHousingDataSet(cal_x_val, cal_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e64c200-1642-4401-a14f-14b029160b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Data Loaders\n",
    "batch_size = 32\n",
    "cal_train_dataloader = torch.utils.data.DataLoader(cal_training, batch_size=batch_size, shuffle=True)\n",
    "cal_val_dataloader = torch.utils.data.DataLoader(cal_validation, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a9f9c85-97ab-4470-89f6-f7c1b34304d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalHousingPredictor(\n",
      "  (sequential): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "class CalHousingPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CalHousingPredictor, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(8, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.sequential(x)\n",
    "        return logits\n",
    "\n",
    "cal_housing_model = CalHousingPredictor()\n",
    "print(cal_housing_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58d4d00b-28b6-47a2-bdea-8c4282e962cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Loss\n",
    "cal_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Choose Optimizer\n",
    "cal_optim_fn = torch.optim.Adam(cal_housing_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3584bd95-b55a-467b-b4d0-6db004c08039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Epoch Functionality\n",
    "# One epoch training function\n",
    "def one_epoch_training_regress(epoch_index, tb_writer):\n",
    "    '''\n",
    "        Parameters:\n",
    "            epoch_index: Specifies the current epoch being trained\n",
    "            tb_writer: Specifies the TensorBoard Writer for visualization\n",
    "\n",
    "        Returns:\n",
    "            Final loss value for epoch\n",
    "    '''\n",
    "    running_loss = 0.\n",
    "    final_loss = 0.\n",
    "    record_batch = len(cal_train_dataloader) // 3\n",
    "    \n",
    "    # Loop through all batches of images \n",
    "    for i, data in enumerate(cal_train_dataloader):\n",
    "        inputs, labels = data\n",
    "        # inputs = inputs.to(device)\n",
    "        # labels = labels.to(device)\n",
    "\n",
    "        cal_optim_fn.zero_grad()\n",
    "\n",
    "        predictions = cal_housing_model(inputs)\n",
    "\n",
    "        loss = cal_loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        cal_optim_fn.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % record_batch == record_batch - 1:\n",
    "            final_loss = running_loss / 1000 # loss per batch\n",
    "            print(f' batch {i+1} lost: {final_loss}')\n",
    "            samples_trained = epoch_index * len(cal_train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', final_loss, samples_trained)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66d1f210-96a6-4f31-b06b-b3b2af24d0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      " batch 121 lost: 0.24227539229393005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blake/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 242 lost: 0.1633368697166443\n",
      " batch 363 lost: 0.16275593996047974\n",
      "LOSS train 0.16275593996047974, valid 1.337125301361084\n",
      "EPOCH 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blake/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/blake/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 121 lost: 0.1556873790025711\n",
      " batch 242 lost: 0.16763301420211793\n",
      " batch 363 lost: 0.16673532247543335\n",
      "LOSS train 0.16673532247543335, valid 1.3183797597885132\n",
      "EPOCH 3:\n",
      " batch 121 lost: 0.16245104098320007\n",
      " batch 242 lost: 0.16276346576213838\n",
      " batch 363 lost: 0.16379873436689377\n",
      "LOSS train 0.16379873436689377, valid 1.3300044536590576\n",
      "EPOCH 4:\n",
      " batch 121 lost: 0.163224317073822\n",
      " batch 242 lost: 0.16303661799430846\n",
      " batch 363 lost: 0.16355230355262756\n",
      "LOSS train 0.16355230355262756, valid 1.327758550643921\n",
      "EPOCH 5:\n",
      " batch 121 lost: 0.16383348923921584\n",
      " batch 242 lost: 0.16200156950950623\n",
      " batch 363 lost: 0.1635093767642975\n",
      "LOSS train 0.1635093767642975, valid 1.3372598886489868\n",
      "EPOCH 6:\n",
      " batch 121 lost: 0.16273825150728224\n",
      " batch 242 lost: 0.16817642033100128\n",
      " batch 363 lost: 0.15876131051778794\n",
      "LOSS train 0.15876131051778794, valid 1.3151535987854004\n",
      "EPOCH 7:\n",
      " batch 121 lost: 0.16758970671892165\n",
      " batch 242 lost: 0.16548723286390304\n",
      " batch 363 lost: 0.1578764048218727\n",
      "LOSS train 0.1578764048218727, valid 1.3150873184204102\n",
      "EPOCH 8:\n",
      " batch 121 lost: 0.16232903039455412\n",
      " batch 242 lost: 0.16390608018636704\n",
      " batch 363 lost: 0.16262529772520065\n",
      "LOSS train 0.16262529772520065, valid 1.3323560953140259\n",
      "EPOCH 9:\n",
      " batch 121 lost: 0.16527204424142838\n",
      " batch 242 lost: 0.16626943898200988\n",
      " batch 363 lost: 0.1577968845963478\n",
      "LOSS train 0.1577968845963478, valid 1.360288381576538\n",
      "EPOCH 10:\n",
      " batch 121 lost: 0.1647632368206978\n",
      " batch 242 lost: 0.1610498713850975\n",
      " batch 363 lost: 0.16367420822381973\n",
      "LOSS train 0.16367420822381973, valid 1.3243321180343628\n",
      "EPOCH 11:\n",
      " batch 121 lost: 0.16408165183663367\n",
      " batch 242 lost: 0.15847345608472824\n",
      " batch 363 lost: 0.16802396780252457\n",
      "LOSS train 0.16802396780252457, valid 1.3661715984344482\n",
      "EPOCH 12:\n",
      " batch 121 lost: 0.16214960438013076\n",
      " batch 242 lost: 0.16652106249332427\n",
      " batch 363 lost: 0.16061738210916518\n",
      "LOSS train 0.16061738210916518, valid 1.3344227075576782\n",
      "EPOCH 13:\n",
      " batch 121 lost: 0.16394515979290009\n",
      " batch 242 lost: 0.16210145330429077\n",
      " batch 363 lost: 0.1639773653149605\n",
      "LOSS train 0.1639773653149605, valid 1.3149373531341553\n",
      "EPOCH 14:\n",
      " batch 121 lost: 0.16485245752334596\n",
      " batch 242 lost: 0.16024504274129867\n",
      " batch 363 lost: 0.16381739926338196\n",
      "LOSS train 0.16381739926338196, valid 1.331756830215454\n",
      "EPOCH 15:\n",
      " batch 121 lost: 0.16119410452246666\n",
      " batch 242 lost: 0.16320965969562531\n",
      " batch 363 lost: 0.16441207629442214\n",
      "LOSS train 0.16441207629442214, valid 1.318837285041809\n",
      "EPOCH 16:\n",
      " batch 121 lost: 0.1674842169880867\n",
      " batch 242 lost: 0.16160372287034988\n",
      " batch 363 lost: 0.16088595145940782\n",
      "LOSS train 0.16088595145940782, valid 1.320326566696167\n",
      "EPOCH 17:\n",
      " batch 121 lost: 0.16665736183524132\n",
      " batch 242 lost: 0.16166354143619538\n",
      " batch 363 lost: 0.16100025272369384\n",
      "LOSS train 0.16100025272369384, valid 1.3449249267578125\n",
      "EPOCH 18:\n",
      " batch 121 lost: 0.16698474317789078\n",
      " batch 242 lost: 0.15985501056909562\n",
      " batch 363 lost: 0.16228403973579406\n",
      "LOSS train 0.16228403973579406, valid 1.3985047340393066\n",
      "EPOCH 19:\n",
      " batch 121 lost: 0.16314536041021346\n",
      " batch 242 lost: 0.1622035943865776\n",
      " batch 363 lost: 0.16495178079605102\n",
      "LOSS train 0.16495178079605102, valid 1.3166251182556152\n",
      "EPOCH 20:\n",
      " batch 121 lost: 0.16549855256080628\n",
      " batch 242 lost: 0.16359867602586747\n",
      " batch 363 lost: 0.16019939631223679\n",
      "LOSS train 0.16019939631223679, valid 1.3219925165176392\n",
      "EPOCH 21:\n",
      " batch 121 lost: 0.16543066698312758\n",
      " batch 242 lost: 0.1652296569943428\n",
      " batch 363 lost: 0.15888424533605575\n",
      "LOSS train 0.15888424533605575, valid 1.314824104309082\n",
      "EPOCH 22:\n",
      " batch 121 lost: 0.16096790540218353\n",
      " batch 242 lost: 0.1616946852207184\n",
      " batch 363 lost: 0.16736495423316955\n",
      "LOSS train 0.16736495423316955, valid 1.3188223838806152\n",
      "EPOCH 23:\n",
      " batch 121 lost: 0.15622370672225952\n",
      " batch 242 lost: 0.15999952191114425\n",
      " batch 363 lost: 0.171731003344059\n",
      "LOSS train 0.171731003344059, valid 1.3148951530456543\n",
      "EPOCH 24:\n",
      " batch 121 lost: 0.16388889694213868\n",
      " batch 242 lost: 0.16183816161751746\n",
      " batch 363 lost: 0.16316531014442442\n",
      "LOSS train 0.16316531014442442, valid 1.3153753280639648\n",
      "EPOCH 25:\n",
      " batch 121 lost: 0.1642053952217102\n",
      " batch 242 lost: 0.15852203917503357\n",
      " batch 363 lost: 0.16653724271059037\n",
      "LOSS train 0.16653724271059037, valid 1.3245943784713745\n",
      "EPOCH 26:\n",
      " batch 121 lost: 0.1592549545764923\n",
      " batch 242 lost: 0.1668822362422943\n",
      " batch 363 lost: 0.16390440809726714\n",
      "LOSS train 0.16390440809726714, valid 1.314767599105835\n",
      "EPOCH 27:\n",
      " batch 121 lost: 0.1642658582031727\n",
      " batch 242 lost: 0.16537601268291474\n",
      " batch 363 lost: 0.15917880553007127\n",
      "LOSS train 0.15917880553007127, valid 1.31477689743042\n",
      "EPOCH 28:\n",
      " batch 121 lost: 0.1649477509856224\n",
      " batch 242 lost: 0.16626722219586373\n",
      " batch 363 lost: 0.15759736251831055\n",
      "LOSS train 0.15759736251831055, valid 1.320245385169983\n",
      "EPOCH 29:\n",
      " batch 121 lost: 0.160500809520483\n",
      " batch 242 lost: 0.16188109993934632\n",
      " batch 363 lost: 0.16703342539072036\n",
      "LOSS train 0.16703342539072036, valid 1.319805383682251\n",
      "EPOCH 30:\n",
      " batch 121 lost: 0.15878336012363434\n",
      " batch 242 lost: 0.16637126737833022\n",
      " batch 363 lost: 0.163961085408926\n",
      "LOSS train 0.163961085408926, valid 1.3151733875274658\n"
     ]
    }
   ],
   "source": [
    "# Actual training loop: Equivalent to tensorflow.fit\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/cal_housing_{}'.format(timestamp))\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "best_vloss = 1_000_000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch+1}:')\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    cal_housing_model.train(True)\n",
    "    avg_loss = one_epoch_training_regress(epoch, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    # Set the model to evaluation mode, disabling dropout and using population statistics for batch normalization\n",
    "    cal_housing_model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption\n",
    "    # Calculate validation loss here\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(cal_val_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            # vinputs = vinputs.to(device)\n",
    "            # vlabels = vlabels.to(device)\n",
    "            vpredictions = cal_housing_model(vinputs)\n",
    "            vloss = cal_loss_fn(vpredictions, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train {avg_loss}, valid {avg_vloss}')\n",
    "\n",
    "    writer.add_scalars('Training vs. Validation Loss', {\n",
    "        'Training' : avg_loss, 'Validation' : avg_vloss\n",
    "    }, epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the best model state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f'Models/CalHousing/cal_housing_model_{timestamp}_{epoch}'\n",
    "        torch.save(cal_housing_model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90f010-807d-4eb3-9a22-d67844a8d1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a6d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
